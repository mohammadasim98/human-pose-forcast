{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-28T09:06:28.152772Z",
     "iopub.status.busy": "2023-07-28T09:06:28.152058Z",
     "iopub.status.idle": "2023-07-28T09:06:31.214764Z",
     "shell.execute_reply": "2023-07-28T09:06:31.213915Z",
     "shell.execute_reply.started": "2023-07-28T09:06:28.152750Z"
    },
    "id": "U_yLHnXsLFzF",
    "outputId": "31480639-ccf5-4009-a866-83843258f3ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tfrecord\n",
      "  Downloading tfrecord-1.14.3-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.13.4)\n",
      "Collecting torch_dct\n",
      "  Downloading torch_dct-0.1.6-py3-none-any.whl (5.1 kB)\n",
      "Collecting crc32c\n",
      "  Downloading crc32c-2.3.post0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from tfrecord) (3.19.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tfrecord) (1.23.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (66.1.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.30)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from torch_dct) (1.12.1+cu116)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->torch_dct) (4.4.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Installing collected packages: crc32c, tfrecord, torch_dct\n",
      "Successfully installed crc32c-2.3.post0 tfrecord-1.14.3 torch_dct-0.1.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tfrecord wandb torch_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T16:26:27.781573Z",
     "iopub.status.busy": "2023-07-28T16:26:27.781292Z",
     "iopub.status.idle": "2023-07-28T16:26:28.344480Z",
     "shell.execute_reply": "2023-07-28T16:26:28.343809Z",
     "shell.execute_reply.started": "2023-07-28T16:26:27.781553Z"
    },
    "id": "PL4UkrhsKhhU"
   },
   "outputs": [],
   "source": [
    "# Change this line if you're using Colab to something like '/content/drive/MyDrive/TeamX/'\n",
    "# where TeamX is just the clone of repository on your Google Drive\n",
    "# and you have mounted the drive at /content/drive\n",
    "# See the Tutorial Slides for more detail.\n",
    "\n",
    "# Works on your local machine but not on Colab!\n",
    "PROJECT_ROOT = '/notebooks'\n",
    "\n",
    "# Fix this path and use this one on Colab\n",
    "# PROJECT_ROOT = '/content/drive/MyDrive/TeamX'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from os.path import join as ospj\n",
    "sys.path.append(ospj(PROJECT_ROOT, 'src'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-07-24T23:39:15.734591Z",
     "iopub.status.busy": "2023-07-24T23:39:15.734247Z",
     "iopub.status.idle": "2023-07-24T23:39:15.908820Z",
     "shell.execute_reply": "2023-07-24T23:39:15.907858Z",
     "shell.execute_reply.started": "2023-07-24T23:39:15.734567Z"
    },
    "id": "GRJWZUMgOCAJ",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "384b37ee-cc73-4c08-a840-655b8f105779"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trainer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "trainer.model.cpu()\n",
    "del trainer.model\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "del train_loader\n",
    "del val_loader\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-07-23T14:36:22.011511Z",
     "iopub.status.busy": "2023-07-23T14:36:22.011232Z",
     "iopub.status.idle": "2023-07-23T14:36:23.297379Z",
     "shell.execute_reply": "2023-07-23T14:36:23.295635Z",
     "shell.execute_reply.started": "2023-07-23T14:36:22.011489Z"
    },
    "id": "xx15rC_qKhhd",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "61c242ca-7c9d-4528-be66-7abb4096196e"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maimport\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-ConfigParser # Due to an issue of pickle and auto_reload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m config \u001b[38;5;241m=\u001b[39m ConfigParser\u001b[38;5;241m.\u001b[39mwo_args(config\u001b[38;5;241m=\u001b[39mospj(PROJECT_ROOT,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcfgs/project/gradient-config-hppw.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 16\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_loader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m datamodule\u001b[38;5;241m.\u001b[39mget_loader()\n\u001b[1;32m     19\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39minit_obj(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_loader\u001b[39m\u001b[38;5;124m'\u001b[39m, module_data)\n",
      "File \u001b[0;32m/notebooks/src/utils/config_parser.py:120\u001b[0m, in \u001b[0;36mConfigParser.init_obj\u001b[0;34m(self, name, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m([k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m module_args \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverwriting kwargs given in config file is not allowed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    119\u001b[0m module_args\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/src/data/threeDPW.py:80\u001b[0m, in \u001b[0;36mThreeDPWTFRecordDataset.__init__\u001b[0;34m(self, data_path, n_scenes, person_id, subsample, history_window, future_window, batch_size, resize, shuffle, n_workers, prefetch_factor, transforms)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture_data_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m transforms\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/src/data/threeDPW.py:126\u001b[0m, in \u001b[0;36mThreeDPWTFRecordDataset._cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Iterate over each scene\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scene_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset:\n\u001b[1;32m    127\u001b[0m     num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Generate windowed representation by adding an extra dimension \u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tfrecord/reader.py:219\u001b[0m, in \u001b[0;36mexample_loader\u001b[0;34m(data_path, index_path, description, shard, compression_type)\u001b[0m\n\u001b[1;32m    206\u001b[0m typename_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbyte\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes_list\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat_list\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64_list\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m }\n\u001b[1;32m    212\u001b[0m record_iterator \u001b[38;5;241m=\u001b[39m tfrecord_iterator(\n\u001b[1;32m    213\u001b[0m     data_path\u001b[38;5;241m=\u001b[39mdata_path,\n\u001b[1;32m    214\u001b[0m     index_path\u001b[38;5;241m=\u001b[39mindex_path,\n\u001b[1;32m    215\u001b[0m     shard\u001b[38;5;241m=\u001b[39mshard,\n\u001b[1;32m    216\u001b[0m     compression_type\u001b[38;5;241m=\u001b[39mcompression_type,\n\u001b[1;32m    217\u001b[0m )\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m record_iterator:\n\u001b[1;32m    220\u001b[0m     example \u001b[38;5;241m=\u001b[39m example_pb2\u001b[38;5;241m.\u001b[39mExample()\n\u001b[1;32m    221\u001b[0m     example\u001b[38;5;241m.\u001b[39mParseFromString(record)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tfrecord/reader.py:80\u001b[0m, in \u001b[0;36mtfrecord_iterator\u001b[0;34m(data_path, index_path, shard, compression_type)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m datum_bytes_view\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m read_records()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(index_path, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tfrecord/reader.py:73\u001b[0m, in \u001b[0;36mtfrecord_iterator.<locals>.read_records\u001b[0;34m(start_offset, end_offset)\u001b[0m\n\u001b[1;32m     71\u001b[0m     datum_bytes \u001b[38;5;241m=\u001b[39m datum_bytes\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;28mint\u001b[39m(length \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.5\u001b[39m))\n\u001b[1;32m     72\u001b[0m datum_bytes_view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(datum_bytes)[:length]\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatum_bytes_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m length:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to read the record.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mreadinto(crc_bytes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Initialize the 2D trainer and model\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trainers.hppw_trainer import HPPWTrainer\n",
    "from utils.config_parser import ConfigParser\n",
    "import data.threeDPW as module_data\n",
    "from utils.io import seed_everything\n",
    "\n",
    "# For fair comparisons\n",
    "seed_everything(100)\n",
    "\n",
    "%aimport -ConfigParser # Due to an issue of pickle and auto_reload\n",
    "config = ConfigParser.wo_args(config=ospj(PROJECT_ROOT,'cfgs/project/gradient-config-hppw.json'))\n",
    "\n",
    "datamodule = config.init_obj('train_loader', module_data)\n",
    "train_loader = datamodule.get_loader()\n",
    "\n",
    "datamodule = config.init_obj('validation_loader', module_data)\n",
    "val_loader = datamodule.get_loader()\n",
    "\n",
    "trainer = HPPWTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n",
    "stats = trainer.train()\n",
    "\n",
    "plt.plot(stats['loss']['train'], label='train')\n",
    "plt.plot(stats['loss']['val'], label='val')\n",
    "plt.title('Classification loss history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T16:26:29.259667Z",
     "iopub.status.busy": "2023-07-28T16:26:29.259342Z",
     "iopub.status.idle": "2023-07-28T16:26:30.863709Z",
     "shell.execute_reply": "2023-07-28T16:26:30.863003Z",
     "shell.execute_reply.started": "2023-07-28T16:26:29.259645Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Initialize the 3D trainer and model\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.config_parser import ConfigParser\n",
    "import data.threeDPW as module_data\n",
    "from utils.io import seed_everything\n",
    "# For fair comparisons\n",
    "# seed_everything(599)\n",
    "\n",
    "%aimport -ConfigParser # Due to an issue of pickle and auto_reload\n",
    "config = ConfigParser.wo_args(config=ospj(PROJECT_ROOT,'cfgs/project/gradient-config-hppw3d.json'))\n",
    "\n",
    "datamodule = config.init_obj('train_loader', module_data)\n",
    "train_loader = datamodule.get_loader()\n",
    "\n",
    "datamodule = config.init_obj('validation_loader', module_data)\n",
    "val_loader = datamodule.get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T16:26:30.865394Z",
     "iopub.status.busy": "2023-07-28T16:26:30.865134Z",
     "iopub.status.idle": "2023-07-28T16:26:47.367894Z",
     "shell.execute_reply": "2023-07-28T16:26:47.366067Z",
     "shell.execute_reply.started": "2023-07-28T16:26:30.865376Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masim-98-12-26\u001b[0m (\u001b[33mteam-17\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/src/notebooks/wandb/run-20230728_162632-3vvth6tc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild/runs/3vvth6tc\" target=\"_blank\">lemon-darkness-367</a></strong> to <a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Start Training Epoch 1/70, lr=0.000085 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 loss2d: 2.353164 vim2d: 455.87933: :   8% 448/5760 [00:10<01:10, 75.13it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhppw3d_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HPPW3DTrainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m HPPW3DTrainer(config\u001b[38;5;241m=\u001b[39mconfig, train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, eval_loader\u001b[38;5;241m=\u001b[39mval_loader)\n\u001b[0;32m----> 4\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss2d\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain2d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss2d\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval2d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/notebooks/src/trainers/base3d.py:146\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting curriculum to history window: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_window\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, future window: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture_window\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m=\u001b[39m epoch\n\u001b[0;32m--> 146\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m states[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss2d\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss2d\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_projection:\n",
      "File \u001b[0;32m/notebooks/src/trainers/hppw3d_trainer.py:227\u001b[0m, in \u001b[0;36mHPPW3DTrainer._train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m#             if self.use_dct:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m#                 dct_m, idct_m = get_dct_matrix(history_poses2d.shape[1])\u001b[39;00m\n\u001b[1;32m    224\u001b[0m                 \n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m#                 history_poses2d = torch.matmul(dct_m[:dct_n, :], history_poses2d.permute())\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m             output2d, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimg_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrelative_pose_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_pose2d_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m                \u001b[49m\u001b[43mroot_joint_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_root_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpose_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_pose_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfuture_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuture_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m                \u001b[49m\u001b[43mis_teacher_forcing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_teacher_forcing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_poses2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfuture_pose_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_pose_mask\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m                \n\u001b[1;32m    240\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dct:\n\u001b[1;32m    241\u001b[0m                 dct_m, idct_m \u001b[38;5;241m=\u001b[39m get_dct_matrix(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture_window)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m/notebooks/src/models/hppw/model.py:293\u001b[0m, in \u001b[0;36mHumanPosePredictorModel.forward\u001b[0;34m(self, img_seq, relative_pose_seq, root_joint_seq, mask, pose_mask, future_window, is_teacher_forcing, future, future_pose_mask)\u001b[0m\n\u001b[1;32m    281\u001b[0m     _, future_poses, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpose_encoding(\n\u001b[1;32m    282\u001b[0m         relative_poses\u001b[38;5;241m=\u001b[39mfuture_pose2d, \n\u001b[1;32m    283\u001b[0m         root_joints\u001b[38;5;241m=\u001b[39mfuture_root2d, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m         pose_mask\u001b[38;5;241m=\u001b[39mfuture_pose_mask\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Autoregressive decoder with \"dual\" conditioning\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Currently uses only combined local and global features. Need to modify it later for further evaluation.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Out Shape: (B, future_window + 1, J, 2)\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m out_poses, decoder_attentions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_poses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_poses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_teacher_forcing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_teacher_forcing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_poses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj(out_poses[:, \u001b[38;5;241m1\u001b[39m:, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]), [image_attentions, pose_attentions, decoder_attentions]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebooks/src/models/pose/decoder.py:318\u001b[0m, in \u001b[0;36mPoseDecoderV2.forward\u001b[0;34m(self, img_encoding, pos_encoding, tgt, future_window, history_window, is_teacher_forcing, future)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# (B, 15, 256)\u001b[39;00m\n\u001b[1;32m    316\u001b[0m global_memory, global_cross_attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_global_input(img_encoding_global, pos_encoding_global)\n\u001b[0;32m--> 318\u001b[0m memory, cross_attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_global_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Auto Regression, no need for masking\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Out Shape: (batch_size, 1, num_poses + 1, E)\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Get the current pose as the initial input to decoder\u001b[39;00m\n\u001b[1;32m    325\u001b[0m inp_poses \u001b[38;5;241m=\u001b[39m tgt[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/activation.py:1153\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1143\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1151\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1153\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:5066\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5065\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5066\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5068\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:4753\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4751\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4752\u001b[0m             b_q, b_kv \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msplit([E, E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m-> 4753\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_q\u001b[49m\u001b[43m)\u001b[49m,) \u001b[38;5;241m+\u001b[39m linear(k, w_kv, b_kv)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4755\u001b[0m     w_q, w_k, w_v \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from trainers.hppw3d_trainer import HPPW3DTrainer\n",
    "\n",
    "trainer = HPPW3DTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n",
    "stats = trainer.train()\n",
    "\n",
    "plt.plot(stats['loss2d']['train'], label='train2d')\n",
    "plt.plot(stats['loss2d']['val'], label='val2d')\n",
    "plt.plot(stats['loss3d']['train'], label='train3d')\n",
    "plt.plot(stats['loss3d']['val'], label='val3d')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mpjpe')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-21T23:41:20.293149Z",
     "iopub.status.busy": "2023-07-21T23:41:20.292553Z",
     "iopub.status.idle": "2023-07-21T23:41:20.561251Z",
     "shell.execute_reply": "2023-07-21T23:41:20.560313Z",
     "shell.execute_reply.started": "2023-07-21T23:41:20.293137Z"
    },
    "id": "3QIsVuEhKhhe"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprettytable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrettyTable\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_parameters\u001b[39m(model):\n\u001b[1;32m      3\u001b[0m     table \u001b[38;5;241m=\u001b[39m PrettyTable([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModules\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "collapsed": true,
    "id": "29KWrFViKhhf",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4e1e17a8-feac-4200-f12b-9cdeb2410560"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/203 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b387e738a317>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/hppw/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, history, future, is_teacher_forcing)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mmemory_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# Get combined* local and global features from sequences of poses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/hppw/model.py\u001b[0m in \u001b[0;36mimage_encoding\u001b[0;34m(self, img_seq, mask, unroll)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;31m# Out Shape: (batch_size*history_window, num_patches + 1, E) and (batch_size*history_window, E)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mmemory_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_global\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# Out Shape: (batch_size, history_window, num_patches + 1, E) and (batch_size, history_window, E)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/vit/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, key_padding_mask)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mcls_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_mask\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mkey_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Take out the CLS token (in fact \"tokens\" because we have a batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/vit/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, key_padding_mask)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Take a subset of pretrained encoder layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Final layer norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/vit/sequential.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, key_padding_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/vit/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, key_padding_mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1206\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5223\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0min_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is False but in_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5224\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5226\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is True but q_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4764\u001b[0m             \u001b[0;31m# self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4765\u001b[0;31m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4766\u001b[0m             \u001b[0;31m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4767\u001b[0m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 14.75 GiB total capacity; 14.32 GiB already allocated; 832.00 KiB free; 14.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "for i, (history, future) in enumerate(tqdm(val_loader)):\n",
    "    output = trainer.model(history, future)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-21T23:41:27.991707Z",
     "iopub.status.busy": "2023-07-21T23:41:27.991117Z",
     "iopub.status.idle": "2023-07-21T23:41:28.032942Z",
     "shell.execute_reply": "2023-07-21T23:41:28.032277Z",
     "shell.execute_reply.started": "2023-07-21T23:41:27.991683Z"
    },
    "id": "WBrHWH-LUI3d",
    "outputId": "7c2d8240-38f0-4a54-e98a-0ec7bfafc41a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T18:39:33.171391Z",
     "iopub.status.busy": "2023-07-22T18:39:33.170972Z",
     "iopub.status.idle": "2023-07-22T18:39:35.735475Z",
     "shell.execute_reply": "2023-07-22T18:39:35.734900Z",
     "shell.execute_reply.started": "2023-07-22T18:39:33.171360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T18:39:35.736983Z",
     "iopub.status.busy": "2023-07-22T18:39:35.736678Z",
     "iopub.status.idle": "2023-07-22T18:39:35.800426Z",
     "shell.execute_reply": "2023-07-22T18:39:35.799859Z",
     "shell.execute_reply.started": "2023-07-22T18:39:35.736976Z"
    },
    "id": "i8oRVKzyKhhf",
    "outputId": "33dd751c-ab3f-4397-98e3-fe09c2ebed17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                                                      Param #\n",
       "====================================================================================================\n",
       "HumanPosePredictorModel                                                     --\n",
       "├─PoseEncoder: 1-1                                                          --\n",
       "│    └─PoseMultiInputSequential: 2-1                                        --\n",
       "│    │    └─PoseEncoderBlock: 3-1                                           658,432\n",
       "│    │    └─PoseEncoderBlock: 3-2                                           658,432\n",
       "│    │    └─PoseEncoderBlock: 3-3                                           658,432\n",
       "│    │    └─PoseEncoderBlock: 3-4                                           658,432\n",
       "│    │    └─PoseEncoderBlock: 3-5                                           658,432\n",
       "│    │    └─PoseEncoderBlock: 3-6                                           658,432\n",
       "├─VisionTransformer: 1-2                                                    768\n",
       "│    └─Conv2d: 2-2                                                          (590,592)\n",
       "│    └─Encoder: 2-3                                                         151,296\n",
       "│    │    └─MultiInputSequential: 3-7                                       (42,527,232)\n",
       "│    │    └─LayerNorm: 3-8                                                  (1,536)\n",
       "│    └─Sequential: 2-4                                                      --\n",
       "│    │    └─Linear: 3-9                                                     (769,000)\n",
       "│    └─Linear: 2-5                                                          (196,864)\n",
       "│    └─MaxPool2d: 2-6                                                       --\n",
       "├─TemporalEncoder: 1-3                                                      --\n",
       "│    └─TemporalMultiInputSequential: 2-7                                    --\n",
       "│    │    └─TemporalEncoderBlock: 3-10                                      1,582,336\n",
       "│    │    └─TemporalEncoderBlock: 3-11                                      1,582,336\n",
       "├─TemporalEncoder: 1-4                                                      --\n",
       "│    └─TemporalMultiInputSequential: 2-8                                    --\n",
       "│    │    └─TemporalEncoderBlock: 3-12                                      1,582,336\n",
       "│    │    └─TemporalEncoderBlock: 3-13                                      1,582,336\n",
       "├─PoseDecoder: 1-5                                                          --\n",
       "│    └─MultiheadAttention: 2-9                                              197,376\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-14                           65,792\n",
       "│    └─TransformerDecoderLayer: 2-10                                        --\n",
       "│    │    └─MultiheadAttention: 3-15                                        263,168\n",
       "│    │    └─MultiheadAttention: 3-16                                        263,168\n",
       "│    │    └─Linear: 3-17                                                    65,792\n",
       "│    │    └─Dropout: 3-18                                                   --\n",
       "│    │    └─Linear: 3-19                                                    65,792\n",
       "│    │    └─LayerNorm: 3-20                                                 512\n",
       "│    │    └─LayerNorm: 3-21                                                 512\n",
       "│    │    └─LayerNorm: 3-22                                                 512\n",
       "│    │    └─Dropout: 3-23                                                   --\n",
       "│    │    └─Dropout: 3-24                                                   --\n",
       "│    │    └─Dropout: 3-25                                                   --\n",
       "│    └─LayerNorm: 2-11                                                      512\n",
       "│    └─MLPBlock: 2-12                                                       --\n",
       "│    │    └─Linear: 3-26                                                    197,376\n",
       "│    │    └─GELU: 3-27                                                      --\n",
       "│    │    └─Dropout: 3-28                                                   --\n",
       "│    │    └─Linear: 3-29                                                    196,864\n",
       "│    │    └─Dropout: 3-30                                                   --\n",
       "│    └─TransformerDecoder: 2-13                                             --\n",
       "│    │    └─ModuleList: 3-31                                                3,956,736\n",
       "│    └─TemporalEncoder: 2-14                                                --\n",
       "│    │    └─TemporalMultiInputSequential: 3-32                              2,110,464\n",
       "├─FourierMLPEncoding: 1-6                                                   --\n",
       "│    └─Sequential: 2-15                                                     --\n",
       "│    │    └─Linear: 3-33                                                    33,152\n",
       "│    │    └─ReLU: 3-34                                                      --\n",
       "│    │    └─Linear: 3-35                                                    33,024\n",
       "├─FourierMLPEncoding: 1-7                                                   --\n",
       "│    └─Sequential: 2-16                                                     --\n",
       "│    │    └─Linear: 3-36                                                    65,920\n",
       "│    │    └─ReLU: 3-37                                                      --\n",
       "│    │    └─Linear: 3-38                                                    33,024\n",
       "├─Linear: 1-8                                                               514\n",
       "====================================================================================================\n",
       "Total params: 62,067,434\n",
       "Trainable params: 17,830,146\n",
       "Non-trainable params: 44,237,288\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "torchinfo.summary(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-21T23:50:45.616334Z",
     "iopub.status.busy": "2023-07-21T23:50:45.615757Z",
     "iopub.status.idle": "2023-07-21T23:50:45.658203Z",
     "shell.execute_reply": "2023-07-21T23:50:45.657339Z",
     "shell.execute_reply.started": "2023-07-21T23:50:45.616306Z"
    }
   },
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T08:45:14.539077Z",
     "iopub.status.busy": "2023-07-27T08:45:14.538539Z",
     "iopub.status.idle": "2023-07-27T08:45:29.501167Z",
     "shell.execute_reply": "2023-07-27T08:45:29.500736Z",
     "shell.execute_reply.started": "2023-07-27T08:45:14.539043Z"
    },
    "id": "qfGMPX6VKhhg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: /notebooks/saved/models/gradient-HPPW3D/0727_002416/per_epoch_model.pth ...\n",
      "Checkpoint loaded.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Perform test\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "datamodule = config.init_obj('test_loader', module_data)\n",
    "test_loader = datamodule.get_loader()\n",
    "\n",
    "checkpoint_dir = '0727_002416' \n",
    "path = ospj(PROJECT_ROOT, f'saved/models/gradient-HPPW3D/{checkpoint_dir}/per_epoch_model.pth')\n",
    "\n",
    "trainer.load_model(path=path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T20:10:12.662823Z",
     "iopub.status.busy": "2023-07-25T20:10:12.662528Z",
     "iopub.status.idle": "2023-07-25T20:20:06.651131Z",
     "shell.execute_reply": "2023-07-25T20:20:06.650676Z",
     "shell.execute_reply.started": "2023-07-25T20:10:12.662795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++> Evaluate at epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval epoch: 1 loss2d: 0.003856 vim2d: 9.89666: : 100% 22624/22624 [09:53<00:00, 38.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++> Evaluate epoch 1 Finished.\n",
      "{'loss2d': 0.053645996388892316, 'vim2d': 28.453605109596523}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_dir = '0724_002135' \n",
    "# path = ospj(PROJECT_ROOT, f'saved/models/gradient-HPPW3D/{checkpoint_dir}/best_model.pth')\n",
    "# trainer.load_model(path)\n",
    "result = trainer.evaluate(loader=test_loader, history_window=15, future_window=15)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T17:43:08.170924Z",
     "iopub.status.busy": "2023-07-22T17:43:08.170283Z",
     "iopub.status.idle": "2023-07-22T17:43:08.582907Z",
     "shell.execute_reply": "2023-07-22T17:43:08.582253Z",
     "shell.execute_reply.started": "2023-07-22T17:43:08.170897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: /notebooks/saved/models/gradient-HPPW3D/0722_144744/last_model.pth ...\n",
      "Checkpoint loaded.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T18:19:53.221289Z",
     "iopub.status.busy": "2023-07-22T18:19:53.220706Z",
     "iopub.status.idle": "2023-07-22T18:19:53.272095Z",
     "shell.execute_reply": "2023-07-22T18:19:53.271415Z",
     "shell.execute_reply.started": "2023-07-22T18:19:53.221264Z"
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "for i in range(frames_per_second * video_duration_seconds):\n",
    "    img = createFrame(i)\n",
    "    writer.append_data(img)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T17:51:58.425874Z",
     "iopub.status.busy": "2023-07-22T17:51:58.425280Z",
     "iopub.status.idle": "2023-07-22T17:52:01.722983Z",
     "shell.execute_reply": "2023-07-22T17:52:01.722190Z",
     "shell.execute_reply.started": "2023-07-22T17:51:58.425850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg\n",
      "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ffmpeg\n",
      "  Building wheel for ffmpeg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=5ebdca24bfc77d02e88e3b286d8a5daa4898a1b49e60671e8faef69c9e38b55b\n",
      "  Stored in directory: /root/.cache/pip/wheels/5a/d9/18/59ee81e4098fc14408ea3d13ae1c472d8380acb97160d2619a\n",
      "Successfully built ffmpeg\n",
      "Installing collected packages: ffmpeg\n",
      "Successfully installed ffmpeg-1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T08:51:21.782884Z",
     "iopub.status.busy": "2023-07-27T08:51:21.781850Z",
     "iopub.status.idle": "2023-07-27T09:17:14.356565Z",
     "shell.execute_reply": "2023-07-27T09:17:14.355798Z",
     "shell.execute_reply.started": "2023-07-27T08:51:21.782845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 707/707 [25:52<00:00,  2.20s/it]\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "from models.hppw.transforms import cvt_absolute_pose, cvt_relative_pose\n",
    "from utils.viz import annotate_pose_2d\n",
    "from utils.viz import annotate_root_2d\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "checkpoint_dir = '0727_002416' \n",
    "\n",
    "\n",
    "\n",
    "folder_path = ospj(PROJECT_ROOT, 'data', \"outputs\", checkpoint_dir)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "name = ospj(folder_path, checkpoint_dir + \".mp4\")\n",
    "\n",
    "fps, duration = 15, 100\n",
    "# p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'mjpeg', '-r', str(fps), '-i', '-', '-vcodec', 'mpeg4', '-qscale', '5', '-r', '24', name], stdin=PIPE)\n",
    "\n",
    "out = cv2.VideoWriter(name,cv2.VideoWriter_fourcc('M','J','P','G'), fps, (224,224))\n",
    "\n",
    "def _generate_key_padding_mask(poses: torch.Tensor) -> torch.Tensor:\n",
    "    mask = torch.where(poses==0.0, 1.0, 0.0)\n",
    "\n",
    "    return torch.sum(mask, dim=-1).bool()\n",
    "\n",
    "for i, (history, future) in enumerate(tqdm(test_loader)):\n",
    "            \n",
    "    img_seq = history[0].float().cuda()\n",
    "    history_pose2d_seq = history[1].float().cuda()\n",
    "    history_root_seq = history[2].float().cuda()\n",
    "    history_mask = history[3].float().cuda()\n",
    "    \n",
    "    # mask = torch.zeros_like(history_pose2d_seq)\n",
    "    # torch.where(x > 0, x, y)\n",
    "    \n",
    "    \n",
    "    history_pose2d_seq = cvt_relative_pose(history_root_seq, history_pose2d_seq)\n",
    "        \n",
    "\n",
    "    history_pose2d_seq /= img_seq.shape[3]\n",
    "    history_root_seq /= img_seq.shape[3]\n",
    "\n",
    "    \n",
    "    history_pose2d_mask = _generate_key_padding_mask(history_pose2d_seq)\n",
    "    history_root_mask = _generate_key_padding_mask(history_root_seq)\n",
    "\n",
    "\n",
    "    history_pose_mask =  torch.cat([history_root_mask.unsqueeze(-1), history_pose2d_mask], dim=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_2d, attentions = trainer.model(img_seq, history_pose2d_seq, history_root_seq, history_mask, history_pose_mask, history_root_seq.shape[1])\n",
    "\n",
    "    pred_root_joints = out_2d[..., 0, :]  * img_seq.shape[3]\n",
    "    pred_relative_poses = out_2d[..., 1:, :]  * img_seq.shape[3]\n",
    "    \n",
    "    pred_abs_poses = cvt_absolute_pose(pred_root_joints, pred_relative_poses)     \n",
    "    \n",
    "    \n",
    "    for sample in range(history[0].shape[0]):\n",
    "        img = img_seq[sample, -1, ...]\n",
    "            \n",
    "        abs_pose = history_pose2d_seq[sample, -1, ...]\n",
    "        root_joint =history_root_seq[sample, -1, ...]\n",
    "        \n",
    "        \n",
    "        abs_pose = abs_pose.unsqueeze(0).to(torch.uint8)\n",
    "        root_joint = root_joint.unsqueeze(0).to(torch.uint8)\n",
    "        # abs_pose = cvt_absolute_pose(root_joint=np.expand_dims(root_joint, 0), norm_pose=np.expand_dims(norm_pose, 0))\n",
    "        annoted_img = annotate_pose_2d(img=img.cpu().numpy(), pose=abs_pose.cpu().numpy(), color=(255, 0, 0), radius=2, thickness=2, text=False)\n",
    "        annoted_img = annotate_root_2d(img=annoted_img, root=root_joint.cpu().numpy(), color=(0, 0, 255), thickness=3)\n",
    "        # cv2.imshow(\"History Image\", img)\n",
    "        # cv2.imshow(\"History Mask\", mask*255)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for j in range(future[0].shape[1]):\n",
    "            \n",
    "            gt_abs_pose = future[0][sample, j, ...]\n",
    "            gt_root_joint = future[1][sample, j, ...]\n",
    "            \n",
    "            gt_abs_pose = gt_abs_pose.unsqueeze(0).to(torch.uint8)\n",
    "            gt_root_joint = gt_root_joint.unsqueeze(0).to(torch.uint8)\n",
    "            \n",
    "            pred_abs = pred_abs_poses[sample, j, ...]\n",
    "            pred_root = pred_root_joints[sample, j, ...]\n",
    "            \n",
    "            pred_abs = pred_abs.unsqueeze(0).to(torch.uint8)\n",
    "            pred_root = pred_root.unsqueeze(0).to(torch.uint8)\n",
    "\n",
    "            # abs_pose = cvt_absolute_pose(root_joint=np.expand_dims(root_joint, 0), norm_pose=np.expand_dims(norm_pose, 0))\n",
    "            annotated_img = annotate_pose_2d(img=img.cpu().numpy(), pose=pred_abs.cpu().numpy(), color=(225, 225, 0), radius=2, thickness=2, text=False)\n",
    "            annotated_img = annotate_root_2d(img=annotated_img, root=pred_root.cpu().numpy(), color=(255, 0, 255), thickness=3)\n",
    "            \n",
    "            annotated_img = annotate_pose_2d(img=annotated_img, pose=gt_abs_pose.cpu().numpy(), color=(0, 255, 0), radius=2, thickness=2, text=False)\n",
    "            annotated_img = annotate_root_2d(img=annotated_img,root=gt_root_joint.cpu().numpy(), color=(0, 255, 255), thickness=3)\n",
    "            # cv2.imwrite(ospj(folder_path, str(i)+\".jpg\"), annoted_img)\n",
    "\n",
    "            annotated_img = annotated_img.astype(np.uint8)\n",
    "            out.write(annotated_img)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "out.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AAaJjowKhhg"
   },
   "outputs": [],
   "source": [
    "from models.temporal.encoder import TemporalEncoder, LocalTemporalEncoderBlock\n",
    "\n",
    "model = TemporalEncoder(\n",
    "    num_layers=3,\n",
    "    num_heads=8,\n",
    "    hidden_dim=256,\n",
    "    mlp_dim=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdXvEna-Khhg"
   },
   "outputs": [],
   "source": [
    "local_feat = torch.randn(size=(32, 15, 197, 256))\n",
    "global_feat = torch.randn(size=(32, 15, 256))\n",
    "\n",
    "local_out, global_out = model(local_feat, global_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gObtAvw7Khhg",
    "outputId": "c8b20db4-d18a-4d69-d9df-81afea09b144"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6335],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6336],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6336],\n",
       "         ...,\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6335],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6335],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6336]],\n",
       "\n",
       "        [[-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         ...,\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401]],\n",
       "\n",
       "        [[-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         ...,\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         ...,\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412]],\n",
       "\n",
       "        [[-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         ...,\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081]],\n",
       "\n",
       "        [[ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         ...,\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI02lLoOKhhg",
    "outputId": "4d1708e1-6d98-4ed6-ceac-0c62b89c9e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "├─MultiInputSequential: 1-1                        [-1, 197, 256]            --\n",
      "|    └─TemporalEncoderBlock: 2-1                   [-1, 14, 197, 256]        --\n",
      "|    |    └─LocalTemporalEncoderBlock: 3-1         [-1, 14, 197, 256]        528,128\n",
      "|    |    └─GlobalTemporalEncoderBlock: 3-2        [-1, 15, 256]             527,104\n",
      "|    └─TemporalEncoderBlock: 2-2                   [-1, 13, 197, 256]        --\n",
      "|    |    └─LocalTemporalEncoderBlock: 3-3         [-1, 13, 197, 256]        528,128\n",
      "|    |    └─GlobalTemporalEncoderBlock: 3-4        [-1, 15, 256]             527,104\n",
      "|    └─TemporalEncoderBlock: 2-3                   [-1, 197, 256]            --\n",
      "|    |    └─LocalTemporalEncoderBlock: 3-5         [-1, 197, 256]            528,128\n",
      "|    |    └─GlobalTemporalEncoderBlock: 3-6        [-1, 15, 256]             527,104\n",
      "====================================================================================================\n",
      "Total params: 3,165,696\n",
      "Trainable params: 3,165,696\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 21.54\n",
      "====================================================================================================\n",
      "Input size (MB): 92.81\n",
      "Forward/backward pass size (MB): 4.02\n",
      "Params size (MB): 12.08\n",
      "Estimated Total Size (MB): 108.91\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "├─MultiInputSequential: 1-1                        [-1, 197, 256]            --\n",
       "|    └─TemporalEncoderBlock: 2-1                   [-1, 14, 197, 256]        --\n",
       "|    |    └─LocalTemporalEncoderBlock: 3-1         [-1, 14, 197, 256]        528,128\n",
       "|    |    └─GlobalTemporalEncoderBlock: 3-2        [-1, 15, 256]             527,104\n",
       "|    └─TemporalEncoderBlock: 2-2                   [-1, 13, 197, 256]        --\n",
       "|    |    └─LocalTemporalEncoderBlock: 3-3         [-1, 13, 197, 256]        528,128\n",
       "|    |    └─GlobalTemporalEncoderBlock: 3-4        [-1, 15, 256]             527,104\n",
       "|    └─TemporalEncoderBlock: 2-3                   [-1, 197, 256]            --\n",
       "|    |    └─LocalTemporalEncoderBlock: 3-5         [-1, 197, 256]            528,128\n",
       "|    |    └─GlobalTemporalEncoderBlock: 3-6        [-1, 15, 256]             527,104\n",
       "====================================================================================================\n",
       "Total params: 3,165,696\n",
       "Trainable params: 3,165,696\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 21.54\n",
       "====================================================================================================\n",
       "Input size (MB): 92.81\n",
       "Forward/backward pass size (MB): 4.02\n",
       "Params size (MB): 12.08\n",
       "Estimated Total Size (MB): 108.91\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_data=[local_feat, global_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDLiJsFrKhhh",
    "outputId": "250e8f61-3e4b-4680-cb43-5f02fab5ec99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: There's no GPU available on this machine,training will be performed on CPU.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrainers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhppw_trainer\u001b[39;00m \u001b[39mimport\u001b[39;00m HPPWTrainer\n\u001b[1;32m----> 7\u001b[0m trainer \u001b[39m=\u001b[39m HPPWTrainer(config\u001b[39m=\u001b[39;49mconfig, train_loader\u001b[39m=\u001b[39;49mtrain_loader, eval_loader\u001b[39m=\u001b[39;49mval_loader)\n\u001b[0;32m      8\u001b[0m \u001b[39m# stats = trainer.train()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m plt\u001b[39m.\u001b[39mplot(stats[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\\../..\\src\\trainers\\hppw_trainer.py:24\u001b[0m, in \u001b[0;36mHPPWTrainer.__init__\u001b[1;34m(self, config, train_loader, eval_loader)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, train_loader, eval_loader\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    Create the model, loss criterion, optimizer, and dataloaders\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m    And anything else that might be needed during training. (e.g. device type)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config)    \n\u001b[0;32m     25\u001b[0m     \u001b[39m# build model architecture, then print to console\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39minit_obj(\u001b[39m'\u001b[39m\u001b[39march\u001b[39m\u001b[39m'\u001b[39m, module_arch)\n",
      "File \u001b[1;32md:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\\../..\\src\\trainers\\base.py:92\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39m# prepare for (multi-device) GPU training\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39m# This part doesn't do anything if you don't have a GPU.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_ids \u001b[39m=\u001b[39m prepare_device(config[\u001b[39m'\u001b[39m\u001b[39mn_gpu\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwandb_enabled \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39;49m\u001b[39mwandb\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwandb_enabled \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mwandb\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\\../..\\src\\utils\\config_parser.py:139\u001b[0m, in \u001b[0;36mConfigParser.__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m    138\u001b[0m     \u001b[39m\"\"\"Access items like ordinary dict.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[name]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'wandb'"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Initialize the trainer and model\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trainers.hppw_trainer import HPPWTrainer\n",
    "\n",
    "trainer = HPPWTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n",
    "# stats = trainer.train()\n",
    "\n",
    "plt.plot(stats['loss']['train'], label='train')\n",
    "plt.plot(stats['loss']['val'], label='val')\n",
    "plt.title('Classification loss history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = HPPW3DTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T17:40:56.442671Z",
     "iopub.status.busy": "2023-07-22T17:40:56.442410Z",
     "iopub.status.idle": "2023-07-22T17:41:05.907969Z",
     "shell.execute_reply": "2023-07-22T17:41:05.907449Z",
     "shell.execute_reply.started": "2023-07-22T17:40:56.442651Z"
    },
    "id": "9R5N7uoRKhhh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jyoihtr0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval_loss2d</td><td>█████████▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_vim2d</td><td>▇▇▇▇█▇▆▇▇▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss2d</td><td>█████████▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>vim2d</td><td>█████████▇▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>eval_loss2d</td><td>0.01851</td></tr><tr><td>eval_vim2d</td><td>22.99053</td></tr><tr><td>loss2d</td><td>0.00901</td></tr><tr><td>vim2d</td><td>12.72046</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cool-glitter-155</strong>: <a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild/runs/jyoihtr0\" target=\"_blank\">https://wandb.ai/team-17/human-pose-prediction-in-the-wild/runs/jyoihtr0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230722_144747-jyoihtr0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jyoihtr0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b37784c8d6b46458e6bbe6bf52ef1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666913165245205, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/src/notebooks/wandb/run-20230722_174056-1zfjmjdc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild/runs/1zfjmjdc\" target=\"_blank\">cool-water-156</a></strong> to <a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 8.5000e-05.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
