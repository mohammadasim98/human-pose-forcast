{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-22T18:19:30.676980Z",
     "iopub.status.busy": "2023-07-22T18:19:30.676470Z",
     "iopub.status.idle": "2023-07-22T18:19:33.221254Z",
     "shell.execute_reply": "2023-07-22T18:19:33.220647Z",
     "shell.execute_reply.started": "2023-07-22T18:19:30.676953Z"
    },
    "id": "U_yLHnXsLFzF",
    "outputId": "31480639-ccf5-4009-a866-83843258f3ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tfrecord in /usr/local/lib/python3.9/dist-packages (1.14.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.13.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tfrecord) (1.23.4)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from tfrecord) (3.19.6)\n",
      "Requirement already satisfied: crc32c in /usr/local/lib/python3.9/dist-packages (from tfrecord) (2.3.post0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (66.1.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.30)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tfrecord wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T14:25:31.382123Z",
     "iopub.status.busy": "2023-07-22T14:25:31.381600Z",
     "iopub.status.idle": "2023-07-22T14:25:31.409038Z",
     "shell.execute_reply": "2023-07-22T14:25:31.408416Z",
     "shell.execute_reply.started": "2023-07-22T14:25:31.382102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0,10,0,16])\n",
    "\n",
    "(~(a == 0)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T18:38:32.353660Z",
     "iopub.status.busy": "2023-07-22T18:38:32.353411Z",
     "iopub.status.idle": "2023-07-22T18:38:32.909557Z",
     "shell.execute_reply": "2023-07-22T18:38:32.909043Z",
     "shell.execute_reply.started": "2023-07-22T18:38:32.353643Z"
    },
    "id": "PL4UkrhsKhhU"
   },
   "outputs": [],
   "source": [
    "# Change this line if you're using Colab to something like '/content/drive/MyDrive/TeamX/'\n",
    "# where TeamX is just the clone of repository on your Google Drive\n",
    "# and you have mounted the drive at /content/drive\n",
    "# See the Tutorial Slides for more detail.\n",
    "\n",
    "# Works on your local machine but not on Colab!\n",
    "PROJECT_ROOT = '/notebooks'\n",
    "\n",
    "# Fix this path and use this one on Colab\n",
    "# PROJECT_ROOT = '/content/drive/MyDrive/TeamX'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from os.path import join as ospj\n",
    "sys.path.append(ospj(PROJECT_ROOT, 'src'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-07-20T08:15:08.258885Z",
     "iopub.status.busy": "2023-07-20T08:15:08.258412Z",
     "iopub.status.idle": "2023-07-20T08:15:08.340349Z",
     "shell.execute_reply": "2023-07-20T08:15:08.338963Z",
     "shell.execute_reply.started": "2023-07-20T08:15:08.258831Z"
    },
    "id": "GRJWZUMgOCAJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "384b37ee-cc73-4c08-a840-655b8f105779"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trainer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "trainer.model.cpu()\n",
    "del trainer.model\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "del train_loader\n",
    "del val_loader\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-22T18:38:37.766174Z",
     "iopub.status.busy": "2023-07-22T18:38:37.765684Z",
     "iopub.status.idle": "2023-07-22T18:38:39.935516Z",
     "shell.execute_reply": "2023-07-22T18:38:39.934727Z",
     "shell.execute_reply.started": "2023-07-22T18:38:37.766168Z"
    },
    "id": "xx15rC_qKhhd",
    "outputId": "61c242ca-7c9d-4528-be66-7abb4096196e"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maimport\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-ConfigParser # Due to an issue of pickle and auto_reload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m config \u001b[38;5;241m=\u001b[39m ConfigParser\u001b[38;5;241m.\u001b[39mwo_args(config\u001b[38;5;241m=\u001b[39mospj(PROJECT_ROOT,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcfgs/project/gradient-config-hppw.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 16\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_loader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m datamodule\u001b[38;5;241m.\u001b[39mget_loader()\n\u001b[1;32m     19\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39minit_obj(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_loader\u001b[39m\u001b[38;5;124m'\u001b[39m, module_data)\n",
      "File \u001b[0;32m/notebooks/src/utils/config_parser.py:120\u001b[0m, in \u001b[0;36mConfigParser.init_obj\u001b[0;34m(self, name, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m([k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m module_args \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverwriting kwargs given in config file is not allowed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    119\u001b[0m module_args\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/src/data/threeDPW.py:80\u001b[0m, in \u001b[0;36mThreeDPWTFRecordDataset.__init__\u001b[0;34m(self, data_path, n_scenes, person_id, subsample, history_window, future_window, batch_size, resize, shuffle, n_workers, prefetch_factor, transforms)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture_data_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m transforms\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/src/data/threeDPW.py:126\u001b[0m, in \u001b[0;36mThreeDPWTFRecordDataset._cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Iterate over each scene\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scene_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset:\n\u001b[1;32m    127\u001b[0m     num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Generate windowed representation by adding an extra dimension \u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tfrecord/reader.py:223\u001b[0m, in \u001b[0;36mexample_loader\u001b[0;34m(data_path, index_path, description, shard, compression_type)\u001b[0m\n\u001b[1;32m    220\u001b[0m example \u001b[38;5;241m=\u001b[39m example_pb2\u001b[38;5;241m.\u001b[39mExample()\n\u001b[1;32m    221\u001b[0m example\u001b[38;5;241m.\u001b[39mParseFromString(record)\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mextract_feature_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypename_mapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tfrecord/reader.py:156\u001b[0m, in \u001b[0;36mextract_feature_dict\u001b[0;34m(features, description, typename_mapping)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_keys:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist (select from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 156\u001b[0m     processed_features[key] \u001b[38;5;241m=\u001b[39m \u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypename_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processed_features\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tfrecord/reader.py:137\u001b[0m, in \u001b[0;36mextract_feature_dict.<locals>.get_value\u001b[0;34m(typename, typename_mapping, key)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_value\u001b[39m(typename, typename_mapping, key):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtypename_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tfrecord/reader.py:116\u001b[0m, in \u001b[0;36mprocess_feature\u001b[0;34m(feature, typename, typename_mapping, key)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(should be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreversed_mapping[inferred_typename]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_typename \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes_list\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 116\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inferred_typename \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat_list\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(value, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Initialize the 2D trainer and model\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trainers.hppw_trainer import HPPWTrainer\n",
    "from utils.config_parser import ConfigParser\n",
    "import data.threeDPW as module_data\n",
    "from utils.io import seed_everything\n",
    "\n",
    "# For fair comparisons\n",
    "seed_everything(100)\n",
    "\n",
    "%aimport -ConfigParser # Due to an issue of pickle and auto_reload\n",
    "config = ConfigParser.wo_args(config=ospj(PROJECT_ROOT,'cfgs/project/gradient-config-hppw.json'))\n",
    "\n",
    "datamodule = config.init_obj('train_loader', module_data)\n",
    "train_loader = datamodule.get_loader()\n",
    "\n",
    "datamodule = config.init_obj('validation_loader', module_data)\n",
    "val_loader = datamodule.get_loader()\n",
    "\n",
    "trainer = HPPWTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n",
    "stats = trainer.train()\n",
    "\n",
    "plt.plot(stats['loss']['train'], label='train')\n",
    "plt.plot(stats['loss']['val'], label='val')\n",
    "plt.title('Classification loss history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T14:47:43.728489Z",
     "iopub.status.busy": "2023-07-22T14:47:43.728168Z",
     "iopub.status.idle": "2023-07-22T17:24:22.165859Z",
     "shell.execute_reply": "2023-07-22T17:24:22.165262Z",
     "shell.execute_reply.started": "2023-07-22T14:47:43.728467Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masim-98-12-26\u001b[0m (\u001b[33mteam-17\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/src/notebooks/wandb/run-20230722_183913-3maxe7hh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild/runs/3maxe7hh\" target=\"_blank\">drawn-waterfall-157</a></strong> to <a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 8.5000e-05.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Initialize the 3D trainer and model\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trainers.hppw3d_trainer import HPPW3DTrainer\n",
    "from utils.config_parser import ConfigParser\n",
    "import data.threeDPW as module_data\n",
    "from utils.io import seed_everything\n",
    "\n",
    "# For fair comparisons\n",
    "# seed_everything(599)\n",
    "\n",
    "%aimport -ConfigParser # Due to an issue of pickle and auto_reload\n",
    "config = ConfigParser.wo_args(config=ospj(PROJECT_ROOT,'cfgs/project/gradient-config-hppw3d.json'))\n",
    "\n",
    "datamodule = config.init_obj('train_loader', module_data)\n",
    "train_loader = datamodule.get_loader()\n",
    "\n",
    "datamodule = config.init_obj('validation_loader', module_data)\n",
    "val_loader = datamodule.get_loader()\n",
    "\n",
    "trainer = HPPW3DTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n",
    "stats = trainer.train()\n",
    "\n",
    "plt.plot(stats['loss2d']['train'], label='train2d')\n",
    "plt.plot(stats['loss2d']['val'], label='val2d')\n",
    "plt.plot(stats['loss3d']['train'], label='train3d')\n",
    "plt.plot(stats['loss3d']['val'], label='va3dl')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mpjpe')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-21T23:41:20.293149Z",
     "iopub.status.busy": "2023-07-21T23:41:20.292553Z",
     "iopub.status.idle": "2023-07-21T23:41:20.561251Z",
     "shell.execute_reply": "2023-07-21T23:41:20.560313Z",
     "shell.execute_reply.started": "2023-07-21T23:41:20.293137Z"
    },
    "id": "3QIsVuEhKhhe"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprettytable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrettyTable\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_parameters\u001b[39m(model):\n\u001b[1;32m      3\u001b[0m     table \u001b[38;5;241m=\u001b[39m PrettyTable([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModules\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "collapsed": true,
    "id": "29KWrFViKhhf",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4e1e17a8-feac-4200-f12b-9cdeb2410560"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/203 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b387e738a317>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/hppw/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, history, future, is_teacher_forcing)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mmemory_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# Get combined* local and global features from sequences of poses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/hppw/model.py\u001b[0m in \u001b[0;36mimage_encoding\u001b[0;34m(self, img_seq, mask, unroll)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;31m# Out Shape: (batch_size*history_window, num_patches + 1, E) and (batch_size*history_window, E)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mmemory_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_global\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# Out Shape: (batch_size, history_window, num_patches + 1, E) and (batch_size, history_window, E)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/vit/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, key_padding_mask)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mcls_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_mask\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mkey_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Take out the CLS token (in fact \"tokens\" because we have a batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/vit/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, key_padding_mask)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Take a subset of pretrained encoder layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Final layer norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/vit/sequential.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, key_padding_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/human-pose-prediction-in-the-wild/src/models/vit/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, key_padding_mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1206\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5223\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0min_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is False but in_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5224\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5226\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is True but q_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4764\u001b[0m             \u001b[0;31m# self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4765\u001b[0;31m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4766\u001b[0m             \u001b[0;31m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4767\u001b[0m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 14.75 GiB total capacity; 14.32 GiB already allocated; 832.00 KiB free; 14.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "for i, (history, future) in enumerate(tqdm(val_loader)):\n",
    "    output = trainer.model(history, future)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-21T23:41:27.991707Z",
     "iopub.status.busy": "2023-07-21T23:41:27.991117Z",
     "iopub.status.idle": "2023-07-21T23:41:28.032942Z",
     "shell.execute_reply": "2023-07-21T23:41:28.032277Z",
     "shell.execute_reply.started": "2023-07-21T23:41:27.991683Z"
    },
    "id": "WBrHWH-LUI3d",
    "outputId": "7c2d8240-38f0-4a54-e98a-0ec7bfafc41a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T18:39:33.171391Z",
     "iopub.status.busy": "2023-07-22T18:39:33.170972Z",
     "iopub.status.idle": "2023-07-22T18:39:35.735475Z",
     "shell.execute_reply": "2023-07-22T18:39:35.734900Z",
     "shell.execute_reply.started": "2023-07-22T18:39:33.171360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T18:39:35.736983Z",
     "iopub.status.busy": "2023-07-22T18:39:35.736678Z",
     "iopub.status.idle": "2023-07-22T18:39:35.800426Z",
     "shell.execute_reply": "2023-07-22T18:39:35.799859Z",
     "shell.execute_reply.started": "2023-07-22T18:39:35.736976Z"
    },
    "id": "i8oRVKzyKhhf",
    "outputId": "33dd751c-ab3f-4397-98e3-fe09c2ebed17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                                                      Param #\n",
       "====================================================================================================\n",
       "HumanPosePredictorModel                                                     --\n",
       "PoseEncoder: 1-1                                                          --\n",
       "    PoseMultiInputSequential: 2-1                                        --\n",
       "        PoseEncoderBlock: 3-1                                           658,432\n",
       "        PoseEncoderBlock: 3-2                                           658,432\n",
       "        PoseEncoderBlock: 3-3                                           658,432\n",
       "        PoseEncoderBlock: 3-4                                           658,432\n",
       "        PoseEncoderBlock: 3-5                                           658,432\n",
       "        PoseEncoderBlock: 3-6                                           658,432\n",
       "VisionTransformer: 1-2                                                    768\n",
       "    Conv2d: 2-2                                                          (590,592)\n",
       "    Encoder: 2-3                                                         151,296\n",
       "        MultiInputSequential: 3-7                                       (42,527,232)\n",
       "        LayerNorm: 3-8                                                  (1,536)\n",
       "    Sequential: 2-4                                                      --\n",
       "        Linear: 3-9                                                     (769,000)\n",
       "    Linear: 2-5                                                          (196,864)\n",
       "    MaxPool2d: 2-6                                                       --\n",
       "TemporalEncoder: 1-3                                                      --\n",
       "    TemporalMultiInputSequential: 2-7                                    --\n",
       "        TemporalEncoderBlock: 3-10                                      1,582,336\n",
       "        TemporalEncoderBlock: 3-11                                      1,582,336\n",
       "TemporalEncoder: 1-4                                                      --\n",
       "    TemporalMultiInputSequential: 2-8                                    --\n",
       "        TemporalEncoderBlock: 3-12                                      1,582,336\n",
       "        TemporalEncoderBlock: 3-13                                      1,582,336\n",
       "PoseDecoder: 1-5                                                          --\n",
       "    MultiheadAttention: 2-9                                              197,376\n",
       "        NonDynamicallyQuantizableLinear: 3-14                           65,792\n",
       "    TransformerDecoderLayer: 2-10                                        --\n",
       "        MultiheadAttention: 3-15                                        263,168\n",
       "        MultiheadAttention: 3-16                                        263,168\n",
       "        Linear: 3-17                                                    65,792\n",
       "        Dropout: 3-18                                                   --\n",
       "        Linear: 3-19                                                    65,792\n",
       "        LayerNorm: 3-20                                                 512\n",
       "        LayerNorm: 3-21                                                 512\n",
       "        LayerNorm: 3-22                                                 512\n",
       "        Dropout: 3-23                                                   --\n",
       "        Dropout: 3-24                                                   --\n",
       "        Dropout: 3-25                                                   --\n",
       "    LayerNorm: 2-11                                                      512\n",
       "    MLPBlock: 2-12                                                       --\n",
       "        Linear: 3-26                                                    197,376\n",
       "        GELU: 3-27                                                      --\n",
       "        Dropout: 3-28                                                   --\n",
       "        Linear: 3-29                                                    196,864\n",
       "        Dropout: 3-30                                                   --\n",
       "    TransformerDecoder: 2-13                                             --\n",
       "        ModuleList: 3-31                                                3,956,736\n",
       "    TemporalEncoder: 2-14                                                --\n",
       "        TemporalMultiInputSequential: 3-32                              2,110,464\n",
       "FourierMLPEncoding: 1-6                                                   --\n",
       "    Sequential: 2-15                                                     --\n",
       "        Linear: 3-33                                                    33,152\n",
       "        ReLU: 3-34                                                      --\n",
       "        Linear: 3-35                                                    33,024\n",
       "FourierMLPEncoding: 1-7                                                   --\n",
       "    Sequential: 2-16                                                     --\n",
       "        Linear: 3-36                                                    65,920\n",
       "        ReLU: 3-37                                                      --\n",
       "        Linear: 3-38                                                    33,024\n",
       "Linear: 1-8                                                               514\n",
       "====================================================================================================\n",
       "Total params: 62,067,434\n",
       "Trainable params: 17,830,146\n",
       "Non-trainable params: 44,237,288\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "torchinfo.summary(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-21T23:50:45.616334Z",
     "iopub.status.busy": "2023-07-21T23:50:45.615757Z",
     "iopub.status.idle": "2023-07-21T23:50:45.658203Z",
     "shell.execute_reply": "2023-07-21T23:50:45.657339Z",
     "shell.execute_reply.started": "2023-07-21T23:50:45.616306Z"
    }
   },
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T17:29:14.048001Z",
     "iopub.status.busy": "2023-07-22T17:29:14.047411Z",
     "iopub.status.idle": "2023-07-22T17:31:50.364120Z",
     "shell.execute_reply": "2023-07-22T17:31:50.363413Z",
     "shell.execute_reply.started": "2023-07-22T17:29:14.047967Z"
    },
    "id": "qfGMPX6VKhhg"
   },
   "outputs": [],
   "source": [
    "\"\"\" Perform test\n",
    "\"\"\"\n",
    "\n",
    "%aimport -ConfigParser # Due to an issue of pickle and auto_reload\n",
    "config = ConfigParser.wo_args(config=ospj(PROJECT_ROOT,'cfgs/project/gradient-config-hppw3d.json'))\n",
    "datamodule = config.init_obj('test_loader', module_data)\n",
    "test_loader = datamodule.get_loader()\n",
    "\n",
    "# checkpoint_dir = '0722_144744' \n",
    "# path = ospj(PROJECT_ROOT, f'saved/models/gradient-HPPW3D/{checkpoint_dir}/last_model.pth')\n",
    "\n",
    "# trainer.load_model(path=path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-07-22T17:41:51.480874Z",
     "iopub.status.busy": "2023-07-22T17:41:51.480596Z",
     "iopub.status.idle": "2023-07-22T17:41:51.546094Z",
     "shell.execute_reply": "2023-07-22T17:41:51.545347Z",
     "shell.execute_reply.started": "2023-07-22T17:41:51.480845Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose_encoder.layers.0.ln_1.weight\n",
      "pose_encoder.layers.0.ln_1.bias\n",
      "pose_encoder.layers.0.self_attention.in_proj_weight\n",
      "pose_encoder.layers.0.self_attention.in_proj_bias\n",
      "pose_encoder.layers.0.self_attention.out_proj.weight\n",
      "pose_encoder.layers.0.self_attention.out_proj.bias\n",
      "pose_encoder.layers.0.ln_2.weight\n",
      "pose_encoder.layers.0.ln_2.bias\n",
      "pose_encoder.layers.0.mlp.0.weight\n",
      "pose_encoder.layers.0.mlp.0.bias\n",
      "pose_encoder.layers.0.mlp.3.weight\n",
      "pose_encoder.layers.0.mlp.3.bias\n",
      "pose_encoder.layers.1.ln_1.weight\n",
      "pose_encoder.layers.1.ln_1.bias\n",
      "pose_encoder.layers.1.self_attention.in_proj_weight\n",
      "pose_encoder.layers.1.self_attention.in_proj_bias\n",
      "pose_encoder.layers.1.self_attention.out_proj.weight\n",
      "pose_encoder.layers.1.self_attention.out_proj.bias\n",
      "pose_encoder.layers.1.ln_2.weight\n",
      "pose_encoder.layers.1.ln_2.bias\n",
      "pose_encoder.layers.1.mlp.0.weight\n",
      "pose_encoder.layers.1.mlp.0.bias\n",
      "pose_encoder.layers.1.mlp.3.weight\n",
      "pose_encoder.layers.1.mlp.3.bias\n",
      "pose_encoder.layers.2.ln_1.weight\n",
      "pose_encoder.layers.2.ln_1.bias\n",
      "pose_encoder.layers.2.self_attention.in_proj_weight\n",
      "pose_encoder.layers.2.self_attention.in_proj_bias\n",
      "pose_encoder.layers.2.self_attention.out_proj.weight\n",
      "pose_encoder.layers.2.self_attention.out_proj.bias\n",
      "pose_encoder.layers.2.ln_2.weight\n",
      "pose_encoder.layers.2.ln_2.bias\n",
      "pose_encoder.layers.2.mlp.0.weight\n",
      "pose_encoder.layers.2.mlp.0.bias\n",
      "pose_encoder.layers.2.mlp.3.weight\n",
      "pose_encoder.layers.2.mlp.3.bias\n",
      "pose_encoder.layers.3.ln_1.weight\n",
      "pose_encoder.layers.3.ln_1.bias\n",
      "pose_encoder.layers.3.self_attention.in_proj_weight\n",
      "pose_encoder.layers.3.self_attention.in_proj_bias\n",
      "pose_encoder.layers.3.self_attention.out_proj.weight\n",
      "pose_encoder.layers.3.self_attention.out_proj.bias\n",
      "pose_encoder.layers.3.ln_2.weight\n",
      "pose_encoder.layers.3.ln_2.bias\n",
      "pose_encoder.layers.3.mlp.0.weight\n",
      "pose_encoder.layers.3.mlp.0.bias\n",
      "pose_encoder.layers.3.mlp.3.weight\n",
      "pose_encoder.layers.3.mlp.3.bias\n",
      "pose_encoder.layers.4.ln_1.weight\n",
      "pose_encoder.layers.4.ln_1.bias\n",
      "pose_encoder.layers.4.self_attention.in_proj_weight\n",
      "pose_encoder.layers.4.self_attention.in_proj_bias\n",
      "pose_encoder.layers.4.self_attention.out_proj.weight\n",
      "pose_encoder.layers.4.self_attention.out_proj.bias\n",
      "pose_encoder.layers.4.ln_2.weight\n",
      "pose_encoder.layers.4.ln_2.bias\n",
      "pose_encoder.layers.4.mlp.0.weight\n",
      "pose_encoder.layers.4.mlp.0.bias\n",
      "pose_encoder.layers.4.mlp.3.weight\n",
      "pose_encoder.layers.4.mlp.3.bias\n",
      "pose_encoder.layers.5.ln_1.weight\n",
      "pose_encoder.layers.5.ln_1.bias\n",
      "pose_encoder.layers.5.self_attention.in_proj_weight\n",
      "pose_encoder.layers.5.self_attention.in_proj_bias\n",
      "pose_encoder.layers.5.self_attention.out_proj.weight\n",
      "pose_encoder.layers.5.self_attention.out_proj.bias\n",
      "pose_encoder.layers.5.ln_2.weight\n",
      "pose_encoder.layers.5.ln_2.bias\n",
      "pose_encoder.layers.5.mlp.0.weight\n",
      "pose_encoder.layers.5.mlp.0.bias\n",
      "pose_encoder.layers.5.mlp.3.weight\n",
      "pose_encoder.layers.5.mlp.3.bias\n",
      "image_encoder.class_token\n",
      "image_encoder.conv_proj.weight\n",
      "image_encoder.conv_proj.bias\n",
      "image_encoder.encoder.pos_embedding\n",
      "image_encoder.encoder.layers.encoder_layer_0.ln_1.weight\n",
      "image_encoder.encoder.layers.encoder_layer_0.ln_1.bias\n",
      "image_encoder.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "image_encoder.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "image_encoder.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "image_encoder.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "image_encoder.encoder.layers.encoder_layer_0.ln_2.weight\n",
      "image_encoder.encoder.layers.encoder_layer_0.ln_2.bias\n",
      "image_encoder.encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "image_encoder.encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "image_encoder.encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "image_encoder.encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "image_encoder.encoder.layers.encoder_layer_1.ln_1.weight\n",
      "image_encoder.encoder.layers.encoder_layer_1.ln_1.bias\n",
      "image_encoder.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "image_encoder.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "image_encoder.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "image_encoder.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "image_encoder.encoder.layers.encoder_layer_1.ln_2.weight\n",
      "image_encoder.encoder.layers.encoder_layer_1.ln_2.bias\n",
      "image_encoder.encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "image_encoder.encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "image_encoder.encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "image_encoder.encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "image_encoder.encoder.layers.encoder_layer_2.ln_1.weight\n",
      "image_encoder.encoder.layers.encoder_layer_2.ln_1.bias\n",
      "image_encoder.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "image_encoder.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "image_encoder.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "image_encoder.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "image_encoder.encoder.layers.encoder_layer_2.ln_2.weight\n",
      "image_encoder.encoder.layers.encoder_layer_2.ln_2.bias\n",
      "image_encoder.encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "image_encoder.encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "image_encoder.encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "image_encoder.encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "image_encoder.encoder.layers.encoder_layer_3.ln_1.weight\n",
      "image_encoder.encoder.layers.encoder_layer_3.ln_1.bias\n",
      "image_encoder.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "image_encoder.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "image_encoder.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "image_encoder.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "image_encoder.encoder.layers.encoder_layer_3.ln_2.weight\n",
      "image_encoder.encoder.layers.encoder_layer_3.ln_2.bias\n",
      "image_encoder.encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "image_encoder.encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "image_encoder.encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "image_encoder.encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "image_encoder.encoder.layers.encoder_layer_4.ln_1.weight\n",
      "image_encoder.encoder.layers.encoder_layer_4.ln_1.bias\n",
      "image_encoder.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "image_encoder.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "image_encoder.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "image_encoder.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "image_encoder.encoder.layers.encoder_layer_4.ln_2.weight\n",
      "image_encoder.encoder.layers.encoder_layer_4.ln_2.bias\n",
      "image_encoder.encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "image_encoder.encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "image_encoder.encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "image_encoder.encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "image_encoder.encoder.layers.encoder_layer_5.ln_1.weight\n",
      "image_encoder.encoder.layers.encoder_layer_5.ln_1.bias\n",
      "image_encoder.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "image_encoder.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "image_encoder.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "image_encoder.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "image_encoder.encoder.layers.encoder_layer_5.ln_2.weight\n",
      "image_encoder.encoder.layers.encoder_layer_5.ln_2.bias\n",
      "image_encoder.encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "image_encoder.encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "image_encoder.encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "image_encoder.encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "image_encoder.encoder.ln.weight\n",
      "image_encoder.encoder.ln.bias\n",
      "image_encoder.heads.head.weight\n",
      "image_encoder.heads.head.bias\n",
      "image_encoder.linear.weight\n",
      "image_encoder.linear.bias\n",
      "im_temporal_encoder.layers.0.local_forward_attention.ln_q.weight\n",
      "im_temporal_encoder.layers.0.local_forward_attention.ln_q.bias\n",
      "im_temporal_encoder.layers.0.local_forward_attention.ln_kv.weight\n",
      "im_temporal_encoder.layers.0.local_forward_attention.ln_kv.bias\n",
      "im_temporal_encoder.layers.0.local_forward_attention.attention.in_proj_weight\n",
      "im_temporal_encoder.layers.0.local_forward_attention.attention.in_proj_bias\n",
      "im_temporal_encoder.layers.0.local_forward_attention.attention.out_proj.weight\n",
      "im_temporal_encoder.layers.0.local_forward_attention.attention.out_proj.bias\n",
      "im_temporal_encoder.layers.0.local_forward_attention.ln_2.weight\n",
      "im_temporal_encoder.layers.0.local_forward_attention.ln_2.bias\n",
      "im_temporal_encoder.layers.0.local_forward_attention.mlp.0.weight\n",
      "im_temporal_encoder.layers.0.local_forward_attention.mlp.0.bias\n",
      "im_temporal_encoder.layers.0.local_forward_attention.mlp.3.weight\n",
      "im_temporal_encoder.layers.0.local_forward_attention.mlp.3.bias\n",
      "im_temporal_encoder.layers.0.local_backward_attention.ln_q.weight\n",
      "im_temporal_encoder.layers.0.local_backward_attention.ln_q.bias\n",
      "im_temporal_encoder.layers.0.local_backward_attention.ln_kv.weight\n",
      "im_temporal_encoder.layers.0.local_backward_attention.ln_kv.bias\n",
      "im_temporal_encoder.layers.0.local_backward_attention.attention.in_proj_weight\n",
      "im_temporal_encoder.layers.0.local_backward_attention.attention.in_proj_bias\n",
      "im_temporal_encoder.layers.0.local_backward_attention.attention.out_proj.weight\n",
      "im_temporal_encoder.layers.0.local_backward_attention.attention.out_proj.bias\n",
      "im_temporal_encoder.layers.0.local_backward_attention.ln_2.weight\n",
      "im_temporal_encoder.layers.0.local_backward_attention.ln_2.bias\n",
      "im_temporal_encoder.layers.0.local_backward_attention.mlp.0.weight\n",
      "im_temporal_encoder.layers.0.local_backward_attention.mlp.0.bias\n",
      "im_temporal_encoder.layers.0.local_backward_attention.mlp.3.weight\n",
      "im_temporal_encoder.layers.0.local_backward_attention.mlp.3.bias\n",
      "im_temporal_encoder.layers.0.global_attention.ln_1.weight\n",
      "im_temporal_encoder.layers.0.global_attention.ln_1.bias\n",
      "im_temporal_encoder.layers.0.global_attention.self_attention.in_proj_weight\n",
      "im_temporal_encoder.layers.0.global_attention.self_attention.in_proj_bias\n",
      "im_temporal_encoder.layers.0.global_attention.self_attention.out_proj.weight\n",
      "im_temporal_encoder.layers.0.global_attention.self_attention.out_proj.bias\n",
      "im_temporal_encoder.layers.0.global_attention.ln_2.weight\n",
      "im_temporal_encoder.layers.0.global_attention.ln_2.bias\n",
      "im_temporal_encoder.layers.0.global_attention.mlp.0.weight\n",
      "im_temporal_encoder.layers.0.global_attention.mlp.0.bias\n",
      "im_temporal_encoder.layers.0.global_attention.mlp.3.weight\n",
      "im_temporal_encoder.layers.0.global_attention.mlp.3.bias\n",
      "im_temporal_encoder.layers.1.local_forward_attention.ln_q.weight\n",
      "im_temporal_encoder.layers.1.local_forward_attention.ln_q.bias\n",
      "im_temporal_encoder.layers.1.local_forward_attention.ln_kv.weight\n",
      "im_temporal_encoder.layers.1.local_forward_attention.ln_kv.bias\n",
      "im_temporal_encoder.layers.1.local_forward_attention.attention.in_proj_weight\n",
      "im_temporal_encoder.layers.1.local_forward_attention.attention.in_proj_bias\n",
      "im_temporal_encoder.layers.1.local_forward_attention.attention.out_proj.weight\n",
      "im_temporal_encoder.layers.1.local_forward_attention.attention.out_proj.bias\n",
      "im_temporal_encoder.layers.1.local_forward_attention.ln_2.weight\n",
      "im_temporal_encoder.layers.1.local_forward_attention.ln_2.bias\n",
      "im_temporal_encoder.layers.1.local_forward_attention.mlp.0.weight\n",
      "im_temporal_encoder.layers.1.local_forward_attention.mlp.0.bias\n",
      "im_temporal_encoder.layers.1.local_forward_attention.mlp.3.weight\n",
      "im_temporal_encoder.layers.1.local_forward_attention.mlp.3.bias\n",
      "im_temporal_encoder.layers.1.local_backward_attention.ln_q.weight\n",
      "im_temporal_encoder.layers.1.local_backward_attention.ln_q.bias\n",
      "im_temporal_encoder.layers.1.local_backward_attention.ln_kv.weight\n",
      "im_temporal_encoder.layers.1.local_backward_attention.ln_kv.bias\n",
      "im_temporal_encoder.layers.1.local_backward_attention.attention.in_proj_weight\n",
      "im_temporal_encoder.layers.1.local_backward_attention.attention.in_proj_bias\n",
      "im_temporal_encoder.layers.1.local_backward_attention.attention.out_proj.weight\n",
      "im_temporal_encoder.layers.1.local_backward_attention.attention.out_proj.bias\n",
      "im_temporal_encoder.layers.1.local_backward_attention.ln_2.weight\n",
      "im_temporal_encoder.layers.1.local_backward_attention.ln_2.bias\n",
      "im_temporal_encoder.layers.1.local_backward_attention.mlp.0.weight\n",
      "im_temporal_encoder.layers.1.local_backward_attention.mlp.0.bias\n",
      "im_temporal_encoder.layers.1.local_backward_attention.mlp.3.weight\n",
      "im_temporal_encoder.layers.1.local_backward_attention.mlp.3.bias\n",
      "im_temporal_encoder.layers.1.global_attention.ln_1.weight\n",
      "im_temporal_encoder.layers.1.global_attention.ln_1.bias\n",
      "im_temporal_encoder.layers.1.global_attention.self_attention.in_proj_weight\n",
      "im_temporal_encoder.layers.1.global_attention.self_attention.in_proj_bias\n",
      "im_temporal_encoder.layers.1.global_attention.self_attention.out_proj.weight\n",
      "im_temporal_encoder.layers.1.global_attention.self_attention.out_proj.bias\n",
      "im_temporal_encoder.layers.1.global_attention.ln_2.weight\n",
      "im_temporal_encoder.layers.1.global_attention.ln_2.bias\n",
      "im_temporal_encoder.layers.1.global_attention.mlp.0.weight\n",
      "im_temporal_encoder.layers.1.global_attention.mlp.0.bias\n",
      "im_temporal_encoder.layers.1.global_attention.mlp.3.weight\n",
      "im_temporal_encoder.layers.1.global_attention.mlp.3.bias\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.ln_q.weight\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.ln_q.bias\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.ln_kv.weight\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.ln_kv.bias\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.attention.in_proj_weight\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.attention.in_proj_bias\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.attention.out_proj.weight\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.attention.out_proj.bias\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.ln_2.weight\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.ln_2.bias\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.mlp.0.weight\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.mlp.0.bias\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.mlp.3.weight\n",
      "pose_temporal_encoder.layers.0.local_forward_attention.mlp.3.bias\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.ln_q.weight\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.ln_q.bias\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.ln_kv.weight\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.ln_kv.bias\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.attention.in_proj_weight\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.attention.in_proj_bias\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.attention.out_proj.weight\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.attention.out_proj.bias\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.ln_2.weight\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.ln_2.bias\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.mlp.0.weight\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.mlp.0.bias\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.mlp.3.weight\n",
      "pose_temporal_encoder.layers.0.local_backward_attention.mlp.3.bias\n",
      "pose_temporal_encoder.layers.0.global_attention.ln_1.weight\n",
      "pose_temporal_encoder.layers.0.global_attention.ln_1.bias\n",
      "pose_temporal_encoder.layers.0.global_attention.self_attention.in_proj_weight\n",
      "pose_temporal_encoder.layers.0.global_attention.self_attention.in_proj_bias\n",
      "pose_temporal_encoder.layers.0.global_attention.self_attention.out_proj.weight\n",
      "pose_temporal_encoder.layers.0.global_attention.self_attention.out_proj.bias\n",
      "pose_temporal_encoder.layers.0.global_attention.ln_2.weight\n",
      "pose_temporal_encoder.layers.0.global_attention.ln_2.bias\n",
      "pose_temporal_encoder.layers.0.global_attention.mlp.0.weight\n",
      "pose_temporal_encoder.layers.0.global_attention.mlp.0.bias\n",
      "pose_temporal_encoder.layers.0.global_attention.mlp.3.weight\n",
      "pose_temporal_encoder.layers.0.global_attention.mlp.3.bias\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.ln_q.weight\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.ln_q.bias\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.ln_kv.weight\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.ln_kv.bias\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.attention.in_proj_weight\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.attention.in_proj_bias\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.attention.out_proj.weight\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.attention.out_proj.bias\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.ln_2.weight\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.ln_2.bias\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.mlp.0.weight\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.mlp.0.bias\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.mlp.3.weight\n",
      "pose_temporal_encoder.layers.1.local_forward_attention.mlp.3.bias\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.ln_q.weight\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.ln_q.bias\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.ln_kv.weight\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.ln_kv.bias\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.attention.in_proj_weight\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.attention.in_proj_bias\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.attention.out_proj.weight\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.attention.out_proj.bias\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.ln_2.weight\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.ln_2.bias\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.mlp.0.weight\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.mlp.0.bias\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.mlp.3.weight\n",
      "pose_temporal_encoder.layers.1.local_backward_attention.mlp.3.bias\n",
      "pose_temporal_encoder.layers.1.global_attention.ln_1.weight\n",
      "pose_temporal_encoder.layers.1.global_attention.ln_1.bias\n",
      "pose_temporal_encoder.layers.1.global_attention.self_attention.in_proj_weight\n",
      "pose_temporal_encoder.layers.1.global_attention.self_attention.in_proj_bias\n",
      "pose_temporal_encoder.layers.1.global_attention.self_attention.out_proj.weight\n",
      "pose_temporal_encoder.layers.1.global_attention.self_attention.out_proj.bias\n",
      "pose_temporal_encoder.layers.1.global_attention.ln_2.weight\n",
      "pose_temporal_encoder.layers.1.global_attention.ln_2.bias\n",
      "pose_temporal_encoder.layers.1.global_attention.mlp.0.weight\n",
      "pose_temporal_encoder.layers.1.global_attention.mlp.0.bias\n",
      "pose_temporal_encoder.layers.1.global_attention.mlp.3.weight\n",
      "pose_temporal_encoder.layers.1.global_attention.mlp.3.bias\n",
      "decoder.process_layer.in_proj_weight\n",
      "decoder.process_layer.in_proj_bias\n",
      "decoder.process_layer.out_proj.weight\n",
      "decoder.process_layer.out_proj.bias\n",
      "decoder.decode_layer.self_attn.in_proj_weight\n",
      "decoder.decode_layer.self_attn.in_proj_bias\n",
      "decoder.decode_layer.self_attn.out_proj.weight\n",
      "decoder.decode_layer.self_attn.out_proj.bias\n",
      "decoder.decode_layer.multihead_attn.in_proj_weight\n",
      "decoder.decode_layer.multihead_attn.in_proj_bias\n",
      "decoder.decode_layer.multihead_attn.out_proj.weight\n",
      "decoder.decode_layer.multihead_attn.out_proj.bias\n",
      "decoder.decode_layer.linear1.weight\n",
      "decoder.decode_layer.linear1.bias\n",
      "decoder.decode_layer.linear2.weight\n",
      "decoder.decode_layer.linear2.bias\n",
      "decoder.decode_layer.norm1.weight\n",
      "decoder.decode_layer.norm1.bias\n",
      "decoder.decode_layer.norm2.weight\n",
      "decoder.decode_layer.norm2.bias\n",
      "decoder.decode_layer.norm3.weight\n",
      "decoder.decode_layer.norm3.bias\n",
      "decoder.ln.weight\n",
      "decoder.ln.bias\n",
      "decoder.mlp.0.weight\n",
      "decoder.mlp.0.bias\n",
      "decoder.mlp.3.weight\n",
      "decoder.mlp.3.bias\n",
      "decoder.decoder.layers.0.self_attn.in_proj_weight\n",
      "decoder.decoder.layers.0.self_attn.in_proj_bias\n",
      "decoder.decoder.layers.0.self_attn.out_proj.weight\n",
      "decoder.decoder.layers.0.self_attn.out_proj.bias\n",
      "decoder.decoder.layers.0.multihead_attn.in_proj_weight\n",
      "decoder.decoder.layers.0.multihead_attn.in_proj_bias\n",
      "decoder.decoder.layers.0.multihead_attn.out_proj.weight\n",
      "decoder.decoder.layers.0.multihead_attn.out_proj.bias\n",
      "decoder.decoder.layers.0.linear1.weight\n",
      "decoder.decoder.layers.0.linear1.bias\n",
      "decoder.decoder.layers.0.linear2.weight\n",
      "decoder.decoder.layers.0.linear2.bias\n",
      "decoder.decoder.layers.0.norm1.weight\n",
      "decoder.decoder.layers.0.norm1.bias\n",
      "decoder.decoder.layers.0.norm2.weight\n",
      "decoder.decoder.layers.0.norm2.bias\n",
      "decoder.decoder.layers.0.norm3.weight\n",
      "decoder.decoder.layers.0.norm3.bias\n",
      "decoder.decoder.layers.1.self_attn.in_proj_weight\n",
      "decoder.decoder.layers.1.self_attn.in_proj_bias\n",
      "decoder.decoder.layers.1.self_attn.out_proj.weight\n",
      "decoder.decoder.layers.1.self_attn.out_proj.bias\n",
      "decoder.decoder.layers.1.multihead_attn.in_proj_weight\n",
      "decoder.decoder.layers.1.multihead_attn.in_proj_bias\n",
      "decoder.decoder.layers.1.multihead_attn.out_proj.weight\n",
      "decoder.decoder.layers.1.multihead_attn.out_proj.bias\n",
      "decoder.decoder.layers.1.linear1.weight\n",
      "decoder.decoder.layers.1.linear1.bias\n",
      "decoder.decoder.layers.1.linear2.weight\n",
      "decoder.decoder.layers.1.linear2.bias\n",
      "decoder.decoder.layers.1.norm1.weight\n",
      "decoder.decoder.layers.1.norm1.bias\n",
      "decoder.decoder.layers.1.norm2.weight\n",
      "decoder.decoder.layers.1.norm2.bias\n",
      "decoder.decoder.layers.1.norm3.weight\n",
      "decoder.decoder.layers.1.norm3.bias\n",
      "decoder.decoder.layers.2.self_attn.in_proj_weight\n",
      "decoder.decoder.layers.2.self_attn.in_proj_bias\n",
      "decoder.decoder.layers.2.self_attn.out_proj.weight\n",
      "decoder.decoder.layers.2.self_attn.out_proj.bias\n",
      "decoder.decoder.layers.2.multihead_attn.in_proj_weight\n",
      "decoder.decoder.layers.2.multihead_attn.in_proj_bias\n",
      "decoder.decoder.layers.2.multihead_attn.out_proj.weight\n",
      "decoder.decoder.layers.2.multihead_attn.out_proj.bias\n",
      "decoder.decoder.layers.2.linear1.weight\n",
      "decoder.decoder.layers.2.linear1.bias\n",
      "decoder.decoder.layers.2.linear2.weight\n",
      "decoder.decoder.layers.2.linear2.bias\n",
      "decoder.decoder.layers.2.norm1.weight\n",
      "decoder.decoder.layers.2.norm1.bias\n",
      "decoder.decoder.layers.2.norm2.weight\n",
      "decoder.decoder.layers.2.norm2.bias\n",
      "decoder.decoder.layers.2.norm3.weight\n",
      "decoder.decoder.layers.2.norm3.bias\n",
      "decoder.decoder.layers.3.self_attn.in_proj_weight\n",
      "decoder.decoder.layers.3.self_attn.in_proj_bias\n",
      "decoder.decoder.layers.3.self_attn.out_proj.weight\n",
      "decoder.decoder.layers.3.self_attn.out_proj.bias\n",
      "decoder.decoder.layers.3.multihead_attn.in_proj_weight\n",
      "decoder.decoder.layers.3.multihead_attn.in_proj_bias\n",
      "decoder.decoder.layers.3.multihead_attn.out_proj.weight\n",
      "decoder.decoder.layers.3.multihead_attn.out_proj.bias\n",
      "decoder.decoder.layers.3.linear1.weight\n",
      "decoder.decoder.layers.3.linear1.bias\n",
      "decoder.decoder.layers.3.linear2.weight\n",
      "decoder.decoder.layers.3.linear2.bias\n",
      "decoder.decoder.layers.3.norm1.weight\n",
      "decoder.decoder.layers.3.norm1.bias\n",
      "decoder.decoder.layers.3.norm2.weight\n",
      "decoder.decoder.layers.3.norm2.bias\n",
      "decoder.decoder.layers.3.norm3.weight\n",
      "decoder.decoder.layers.3.norm3.bias\n",
      "decoder.decoder.layers.4.self_attn.in_proj_weight\n",
      "decoder.decoder.layers.4.self_attn.in_proj_bias\n",
      "decoder.decoder.layers.4.self_attn.out_proj.weight\n",
      "decoder.decoder.layers.4.self_attn.out_proj.bias\n",
      "decoder.decoder.layers.4.multihead_attn.in_proj_weight\n",
      "decoder.decoder.layers.4.multihead_attn.in_proj_bias\n",
      "decoder.decoder.layers.4.multihead_attn.out_proj.weight\n",
      "decoder.decoder.layers.4.multihead_attn.out_proj.bias\n",
      "decoder.decoder.layers.4.linear1.weight\n",
      "decoder.decoder.layers.4.linear1.bias\n",
      "decoder.decoder.layers.4.linear2.weight\n",
      "decoder.decoder.layers.4.linear2.bias\n",
      "decoder.decoder.layers.4.norm1.weight\n",
      "decoder.decoder.layers.4.norm1.bias\n",
      "decoder.decoder.layers.4.norm2.weight\n",
      "decoder.decoder.layers.4.norm2.bias\n",
      "decoder.decoder.layers.4.norm3.weight\n",
      "decoder.decoder.layers.4.norm3.bias\n",
      "decoder.decoder.layers.5.self_attn.in_proj_weight\n",
      "decoder.decoder.layers.5.self_attn.in_proj_bias\n",
      "decoder.decoder.layers.5.self_attn.out_proj.weight\n",
      "decoder.decoder.layers.5.self_attn.out_proj.bias\n",
      "decoder.decoder.layers.5.multihead_attn.in_proj_weight\n",
      "decoder.decoder.layers.5.multihead_attn.in_proj_bias\n",
      "decoder.decoder.layers.5.multihead_attn.out_proj.weight\n",
      "decoder.decoder.layers.5.multihead_attn.out_proj.bias\n",
      "decoder.decoder.layers.5.linear1.weight\n",
      "decoder.decoder.layers.5.linear1.bias\n",
      "decoder.decoder.layers.5.linear2.weight\n",
      "decoder.decoder.layers.5.linear2.bias\n",
      "decoder.decoder.layers.5.norm1.weight\n",
      "decoder.decoder.layers.5.norm1.bias\n",
      "decoder.decoder.layers.5.norm2.weight\n",
      "decoder.decoder.layers.5.norm2.bias\n",
      "decoder.decoder.layers.5.norm3.weight\n",
      "decoder.decoder.layers.5.norm3.bias\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.ln_q.weight\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.ln_q.bias\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.ln_kv.weight\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.ln_kv.bias\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.attention.in_proj_weight\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.attention.in_proj_bias\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.attention.out_proj.weight\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.attention.out_proj.bias\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.ln_2.weight\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.ln_2.bias\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.mlp.0.weight\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.mlp.0.bias\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.mlp.3.weight\n",
      "decoder.temporal_encoder.layers.0.local_forward_attention.mlp.3.bias\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.ln_q.weight\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.ln_q.bias\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.ln_kv.weight\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.ln_kv.bias\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.attention.in_proj_weight\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.attention.in_proj_bias\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.attention.out_proj.weight\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.attention.out_proj.bias\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.ln_2.weight\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.ln_2.bias\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.mlp.0.weight\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.mlp.0.bias\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.mlp.3.weight\n",
      "decoder.temporal_encoder.layers.0.local_backward_attention.mlp.3.bias\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.ln_q.weight\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.ln_q.bias\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.ln_kv.weight\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.ln_kv.bias\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.attention.in_proj_weight\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.attention.in_proj_bias\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.attention.out_proj.weight\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.attention.out_proj.bias\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.ln_2.weight\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.ln_2.bias\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.mlp.0.weight\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.mlp.0.bias\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.mlp.3.weight\n",
      "decoder.temporal_encoder.layers.1.local_forward_attention.mlp.3.bias\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.ln_q.weight\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.ln_q.bias\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.ln_kv.weight\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.ln_kv.bias\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.attention.in_proj_weight\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.attention.in_proj_bias\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.attention.out_proj.weight\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.attention.out_proj.bias\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.ln_2.weight\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.ln_2.bias\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.mlp.0.weight\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.mlp.0.bias\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.mlp.3.weight\n",
      "decoder.temporal_encoder.layers.1.local_backward_attention.mlp.3.bias\n",
      "embed_root.ff.0.weight\n",
      "embed_root.ff.0.bias\n",
      "embed_root.ff.2.weight\n",
      "embed_root.ff.2.bias\n",
      "embed_relative_pose.ff.0.weight\n",
      "embed_relative_pose.ff.0.bias\n",
      "embed_relative_pose.ff.2.weight\n",
      "embed_relative_pose.ff.2.bias\n",
      "linear.weight\n",
      "linear.bias\n"
     ]
    }
   ],
   "source": [
    "result = trainer.evaluate(loader=test_loader)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T17:43:08.170924Z",
     "iopub.status.busy": "2023-07-22T17:43:08.170283Z",
     "iopub.status.idle": "2023-07-22T17:43:08.582907Z",
     "shell.execute_reply": "2023-07-22T17:43:08.582253Z",
     "shell.execute_reply.started": "2023-07-22T17:43:08.170897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: /notebooks/saved/models/gradient-HPPW3D/0722_144744/last_model.pth ...\n",
      "Checkpoint loaded.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T18:19:53.221289Z",
     "iopub.status.busy": "2023-07-22T18:19:53.220706Z",
     "iopub.status.idle": "2023-07-22T18:19:53.272095Z",
     "shell.execute_reply": "2023-07-22T18:19:53.271415Z",
     "shell.execute_reply.started": "2023-07-22T18:19:53.221264Z"
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "for i in range(frames_per_second * video_duration_seconds):\n",
    "    img = createFrame(i)\n",
    "    writer.append_data(img)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T17:51:58.425874Z",
     "iopub.status.busy": "2023-07-22T17:51:58.425280Z",
     "iopub.status.idle": "2023-07-22T17:52:01.722983Z",
     "shell.execute_reply": "2023-07-22T17:52:01.722190Z",
     "shell.execute_reply.started": "2023-07-22T17:51:58.425850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg\n",
      "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ffmpeg\n",
      "  Building wheel for ffmpeg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=5ebdca24bfc77d02e88e3b286d8a5daa4898a1b49e60671e8faef69c9e38b55b\n",
      "  Stored in directory: /root/.cache/pip/wheels/5a/d9/18/59ee81e4098fc14408ea3d13ae1c472d8380acb97160d2619a\n",
      "Successfully built ffmpeg\n",
      "Installing collected packages: ffmpeg\n",
      "Successfully installed ffmpeg-1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T18:41:05.317446Z",
     "iopub.status.busy": "2023-07-22T18:41:05.316937Z",
     "iopub.status.idle": "2023-07-22T19:00:13.590727Z",
     "shell.execute_reply": "2023-07-22T19:00:13.589966Z",
     "shell.execute_reply.started": "2023-07-22T18:41:05.317418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: /notebooks/saved/models/gradient-HPPW3D/0722_144744/last_model.pth ...\n",
      "Checkpoint loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "  6%|         | 48/799 [01:12<17:48,  1.42s/it] wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "  7%|         | 55/799 [01:22<17:44,  1.43s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "  8%|         | 62/799 [01:32<17:26,  1.42s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "  9%|         | 70/799 [01:43<17:06,  1.41s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 10%|         | 76/799 [01:52<17:33,  1.46s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 10%|         | 83/799 [02:02<17:12,  1.44s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 11%|        | 90/799 [02:12<16:51,  1.43s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 12%|        | 97/799 [02:22<16:00,  1.37s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 13%|        | 105/799 [02:32<15:35,  1.35s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 14%|        | 112/799 [02:42<15:49,  1.38s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 15%|        | 119/799 [02:52<15:23,  1.36s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 16%|        | 127/799 [03:02<15:03,  1.34s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 19%|        | 148/799 [03:33<15:56,  1.47s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 21%|        | 168/799 [04:02<14:40,  1.40s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 24%|       | 189/799 [04:32<14:44,  1.45s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 26%|       | 210/799 [05:02<14:12,  1.45s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 29%|       | 231/799 [05:32<13:45,  1.45s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 32%|      | 252/799 [06:02<13:04,  1.43s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 34%|      | 273/799 [06:32<12:36,  1.44s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 37%|      | 294/799 [07:03<12:07,  1.44s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 39%|      | 315/799 [07:32<11:34,  1.44s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 42%|     | 336/799 [08:02<10:45,  1.39s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 45%|     | 357/799 [08:32<10:20,  1.40s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 47%|     | 378/799 [09:02<10:08,  1.45s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 50%|     | 399/799 [09:32<09:37,  1.44s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 53%|    | 420/799 [10:03<09:19,  1.48s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 55%|    | 441/799 [10:33<08:35,  1.44s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 58%|    | 461/799 [11:03<08:11,  1.45s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 60%|    | 482/799 [11:33<07:32,  1.43s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 63%|   | 503/799 [12:03<07:03,  1.43s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 66%|   | 524/799 [12:33<06:48,  1.49s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 68%|   | 544/799 [13:03<06:10,  1.45s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 71%|   | 565/799 [13:32<05:27,  1.40s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 73%|  | 586/799 [14:03<05:10,  1.46s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 76%|  | 607/799 [14:33<04:41,  1.47s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 79%|  | 628/799 [15:04<03:59,  1.40s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 81%|  | 649/799 [15:34<03:30,  1.40s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 84%| | 670/799 [16:04<03:06,  1.45s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 86%| | 691/799 [16:33<02:31,  1.40s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 89%| | 712/799 [17:03<02:03,  1.42s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 92%|| 733/799 [17:33<01:33,  1.42s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 94%|| 754/799 [18:03<01:04,  1.43s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 97%|| 775/799 [18:33<00:34,  1.45s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "100%|| 796/799 [19:04<00:04,  1.47s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "100%|| 799/799 [19:07<00:00,  1.44s/it]\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "from models.hppw.transforms import cvt_absolute_pose, cvt_relative_pose\n",
    "from utils.viz import annotate_pose_2d\n",
    "from utils.viz import annotate_root_2d\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "checkpoint_dir = '0722_144744' \n",
    "path = ospj(PROJECT_ROOT, f'saved/models/gradient-HPPW3D/{checkpoint_dir}/last_model.pth')\n",
    "\n",
    "trainer.load_model(path)\n",
    "\n",
    "folder_path = ospj(PROJECT_ROOT, 'data', \"outputs\", checkpoint_dir)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "name = ospj(folder_path, checkpoint_dir + \".mp4\")\n",
    "\n",
    "fps, duration = 30, 100\n",
    "# p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'mjpeg', '-r', str(fps), '-i', '-', '-vcodec', 'mpeg4', '-qscale', '5', '-r', '24', name], stdin=PIPE)\n",
    "\n",
    "out = cv2.VideoWriter(name,cv2.VideoWriter_fourcc('M','J','P','G'), fps, (224,224))\n",
    "\n",
    "\n",
    "for i, (history, future) in enumerate(tqdm(test_loader)):\n",
    "            \n",
    "    img_seq = history[0].float().cuda()\n",
    "    history_pose2d_seq = history[1].float().cuda()\n",
    "    history_root_seq = history[2].float().cuda()\n",
    "    history_mask = history[3].float().cuda()\n",
    "    \n",
    "    # mask = torch.zeros_like(history_pose2d_seq)\n",
    "    # torch.where(x > 0, x, y)\n",
    "    \n",
    "    \n",
    "    history_pose2d_seq = cvt_relative_pose(history_root_seq, history_pose2d_seq)\n",
    "        \n",
    "\n",
    "    history_pose2d_seq /= img_seq.shape[3]\n",
    "    history_root_seq /= img_seq.shape[3]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_2d, attentions = trainer.model(img_seq, history_pose2d_seq, history_root_seq, history_mask)\n",
    "\n",
    "    pred_root_joints = out_2d[..., 0, :]  * img_seq.shape[3]\n",
    "    pred_relative_poses = out_2d[..., 1:, :]  * img_seq.shape[3]\n",
    "    \n",
    "    pred_abs_poses = cvt_absolute_pose(pred_root_joints, pred_relative_poses)     \n",
    "    \n",
    "    \n",
    "    for sample in range(history[0].shape[0]):\n",
    "        img = img_seq[sample, -1, ...]\n",
    "            \n",
    "        abs_pose = history_pose2d_seq[sample, -1, ...]\n",
    "        root_joint =history_root_seq[sample, -1, ...]\n",
    "        \n",
    "        \n",
    "        abs_pose = abs_pose.unsqueeze(0).to(int)\n",
    "        root_joint = root_joint.unsqueeze(0).to(int)\n",
    "        # abs_pose = cvt_absolute_pose(root_joint=np.expand_dims(root_joint, 0), norm_pose=np.expand_dims(norm_pose, 0))\n",
    "        annoted_img = annotate_pose_2d(img=img.cpu().numpy(), pose=abs_pose.cpu().numpy(), color=(255, 0, 0), radius=2, thickness=2, text=False)\n",
    "        annoted_img = annotate_root_2d(img=annoted_img, root=root_joint.cpu().numpy(), color=(0, 0, 255), thickness=3)\n",
    "        # cv2.imshow(\"History Image\", img)\n",
    "        # cv2.imshow(\"History Mask\", mask*255)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for j in range(future[0].shape[1]):\n",
    "            \n",
    "            gt_abs_pose = future[0][sample, j, ...]\n",
    "            gt_root_joint = future[1][sample, j, ...]\n",
    "            \n",
    "            gt_abs_pose = gt_abs_pose.unsqueeze(0).to(int)\n",
    "            gt_root_joint = gt_root_joint.unsqueeze(0).to(int)\n",
    "            \n",
    "            pred_abs = pred_abs_poses[sample, j, ...]\n",
    "            pred_root = pred_root_joints[sample, j, ...]\n",
    "            \n",
    "            pred_abs = pred_abs.unsqueeze(0).to(int)\n",
    "            pred_root = pred_root.unsqueeze(0).to(int)\n",
    "\n",
    "            # abs_pose = cvt_absolute_pose(root_joint=np.expand_dims(root_joint, 0), norm_pose=np.expand_dims(norm_pose, 0))\n",
    "            annotated_img = annotate_pose_2d(img=img.cpu().numpy(), pose=pred_abs.cpu().numpy(), color=(225, 225, 0), radius=2, thickness=2, text=False)\n",
    "            annotated_img = annotate_root_2d(img=annotated_img, root=pred_root.cpu().numpy(), color=(255, 0, 255), thickness=3)\n",
    "            \n",
    "            annotated_img = annotate_pose_2d(img=annotated_img, pose=gt_abs_pose.cpu().numpy(), color=(0, 255, 0), radius=2, thickness=2, text=False)\n",
    "            annotated_img = annotate_root_2d(img=annotated_img,root=gt_root_joint.cpu().numpy(), color=(0, 255, 255), thickness=3)\n",
    "            # cv2.imwrite(ospj(folder_path, str(i)+\".jpg\"), annoted_img)\n",
    "\n",
    "            annotated_img = annotated_img.astype(np.uint8)\n",
    "            out.write(annotated_img)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "out.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AAaJjowKhhg"
   },
   "outputs": [],
   "source": [
    "from models.temporal.encoder import TemporalEncoder, LocalTemporalEncoderBlock\n",
    "\n",
    "model = TemporalEncoder(\n",
    "    num_layers=3,\n",
    "    num_heads=8,\n",
    "    hidden_dim=256,\n",
    "    mlp_dim=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdXvEna-Khhg"
   },
   "outputs": [],
   "source": [
    "local_feat = torch.randn(size=(32, 15, 197, 256))\n",
    "global_feat = torch.randn(size=(32, 15, 256))\n",
    "\n",
    "local_out, global_out = model(local_feat, global_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gObtAvw7Khhg",
    "outputId": "c8b20db4-d18a-4d69-d9df-81afea09b144"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6335],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6336],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6336],\n",
       "         ...,\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6335],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6335],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6336]],\n",
       "\n",
       "        [[-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         ...,\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401]],\n",
       "\n",
       "        [[-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         ...,\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         ...,\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412]],\n",
       "\n",
       "        [[-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         ...,\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081]],\n",
       "\n",
       "        [[ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         ...,\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI02lLoOKhhg",
    "outputId": "4d1708e1-6d98-4ed6-ceac-0c62b89c9e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "MultiInputSequential: 1-1                        [-1, 197, 256]            --\n",
      "|    TemporalEncoderBlock: 2-1                   [-1, 14, 197, 256]        --\n",
      "|    |    LocalTemporalEncoderBlock: 3-1         [-1, 14, 197, 256]        528,128\n",
      "|    |    GlobalTemporalEncoderBlock: 3-2        [-1, 15, 256]             527,104\n",
      "|    TemporalEncoderBlock: 2-2                   [-1, 13, 197, 256]        --\n",
      "|    |    LocalTemporalEncoderBlock: 3-3         [-1, 13, 197, 256]        528,128\n",
      "|    |    GlobalTemporalEncoderBlock: 3-4        [-1, 15, 256]             527,104\n",
      "|    TemporalEncoderBlock: 2-3                   [-1, 197, 256]            --\n",
      "|    |    LocalTemporalEncoderBlock: 3-5         [-1, 197, 256]            528,128\n",
      "|    |    GlobalTemporalEncoderBlock: 3-6        [-1, 15, 256]             527,104\n",
      "====================================================================================================\n",
      "Total params: 3,165,696\n",
      "Trainable params: 3,165,696\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 21.54\n",
      "====================================================================================================\n",
      "Input size (MB): 92.81\n",
      "Forward/backward pass size (MB): 4.02\n",
      "Params size (MB): 12.08\n",
      "Estimated Total Size (MB): 108.91\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "MultiInputSequential: 1-1                        [-1, 197, 256]            --\n",
       "|    TemporalEncoderBlock: 2-1                   [-1, 14, 197, 256]        --\n",
       "|    |    LocalTemporalEncoderBlock: 3-1         [-1, 14, 197, 256]        528,128\n",
       "|    |    GlobalTemporalEncoderBlock: 3-2        [-1, 15, 256]             527,104\n",
       "|    TemporalEncoderBlock: 2-2                   [-1, 13, 197, 256]        --\n",
       "|    |    LocalTemporalEncoderBlock: 3-3         [-1, 13, 197, 256]        528,128\n",
       "|    |    GlobalTemporalEncoderBlock: 3-4        [-1, 15, 256]             527,104\n",
       "|    TemporalEncoderBlock: 2-3                   [-1, 197, 256]            --\n",
       "|    |    LocalTemporalEncoderBlock: 3-5         [-1, 197, 256]            528,128\n",
       "|    |    GlobalTemporalEncoderBlock: 3-6        [-1, 15, 256]             527,104\n",
       "====================================================================================================\n",
       "Total params: 3,165,696\n",
       "Trainable params: 3,165,696\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 21.54\n",
       "====================================================================================================\n",
       "Input size (MB): 92.81\n",
       "Forward/backward pass size (MB): 4.02\n",
       "Params size (MB): 12.08\n",
       "Estimated Total Size (MB): 108.91\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_data=[local_feat, global_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDLiJsFrKhhh",
    "outputId": "250e8f61-3e4b-4680-cb43-5f02fab5ec99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: There's no GPU available on this machine,training will be performed on CPU.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrainers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhppw_trainer\u001b[39;00m \u001b[39mimport\u001b[39;00m HPPWTrainer\n\u001b[1;32m----> 7\u001b[0m trainer \u001b[39m=\u001b[39m HPPWTrainer(config\u001b[39m=\u001b[39;49mconfig, train_loader\u001b[39m=\u001b[39;49mtrain_loader, eval_loader\u001b[39m=\u001b[39;49mval_loader)\n\u001b[0;32m      8\u001b[0m \u001b[39m# stats = trainer.train()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m plt\u001b[39m.\u001b[39mplot(stats[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\\../..\\src\\trainers\\hppw_trainer.py:24\u001b[0m, in \u001b[0;36mHPPWTrainer.__init__\u001b[1;34m(self, config, train_loader, eval_loader)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, train_loader, eval_loader\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    Create the model, loss criterion, optimizer, and dataloaders\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m    And anything else that might be needed during training. (e.g. device type)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config)    \n\u001b[0;32m     25\u001b[0m     \u001b[39m# build model architecture, then print to console\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39minit_obj(\u001b[39m'\u001b[39m\u001b[39march\u001b[39m\u001b[39m'\u001b[39m, module_arch)\n",
      "File \u001b[1;32md:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\\../..\\src\\trainers\\base.py:92\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39m# prepare for (multi-device) GPU training\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39m# This part doesn't do anything if you don't have a GPU.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_ids \u001b[39m=\u001b[39m prepare_device(config[\u001b[39m'\u001b[39m\u001b[39mn_gpu\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwandb_enabled \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39;49m\u001b[39mwandb\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwandb_enabled \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mwandb\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\\../..\\src\\utils\\config_parser.py:139\u001b[0m, in \u001b[0;36mConfigParser.__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m    138\u001b[0m     \u001b[39m\"\"\"Access items like ordinary dict.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[name]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'wandb'"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Initialize the trainer and model\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trainers.hppw_trainer import HPPWTrainer\n",
    "\n",
    "trainer = HPPWTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n",
    "# stats = trainer.train()\n",
    "\n",
    "plt.plot(stats['loss']['train'], label='train')\n",
    "plt.plot(stats['loss']['val'], label='val')\n",
    "plt.title('Classification loss history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = HPPW3DTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T17:40:56.442671Z",
     "iopub.status.busy": "2023-07-22T17:40:56.442410Z",
     "iopub.status.idle": "2023-07-22T17:41:05.907969Z",
     "shell.execute_reply": "2023-07-22T17:41:05.907449Z",
     "shell.execute_reply.started": "2023-07-22T17:40:56.442651Z"
    },
    "id": "9R5N7uoRKhhh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jyoihtr0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>eval_loss2d</td><td></td></tr><tr><td>eval_vim2d</td><td></td></tr><tr><td>loss2d</td><td></td></tr><tr><td>vim2d</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>eval_loss2d</td><td>0.01851</td></tr><tr><td>eval_vim2d</td><td>22.99053</td></tr><tr><td>loss2d</td><td>0.00901</td></tr><tr><td>vim2d</td><td>12.72046</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cool-glitter-155</strong>: <a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild/runs/jyoihtr0\" target=\"_blank\">https://wandb.ai/team-17/human-pose-prediction-in-the-wild/runs/jyoihtr0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230722_144747-jyoihtr0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jyoihtr0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b37784c8d6b46458e6bbe6bf52ef1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666913165245205, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/src/notebooks/wandb/run-20230722_174056-1zfjmjdc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild/runs/1zfjmjdc\" target=\"_blank\">cool-water-156</a></strong> to <a href=\"https://wandb.ai/team-17/human-pose-prediction-in-the-wild\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 8.5000e-05.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
