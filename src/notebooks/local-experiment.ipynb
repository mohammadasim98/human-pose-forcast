{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this line if you're using Colab to something like '/content/drive/MyDrive/TeamX/'\n",
    "# where TeamX is just the clone of repository on your Google Drive\n",
    "# and you have mounted the drive at /content/drive  \n",
    "# See the Tutorial Slides for more detail.\n",
    "\n",
    "# Works on your local machine but not on Colab!\n",
    "PROJECT_ROOT = '../..' \n",
    "\n",
    "# Fix this path and use this one on Colab\n",
    "# PROJECT_ROOT = '/content/drive/MyDrive/TeamX'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from os.path import join as ospj\n",
    "sys.path.append(ospj(PROJECT_ROOT, 'src'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The syntax of the command is incorrect.\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\"\n",
    "\n",
    "# Change the relative paths below to absolute if running on Colab\n",
    "!mkdir -p ../../saved/models/vit\n",
    "!mv vit_b_16-c867db91.pth ../../saved/models/vit/vit.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Setup input pipeline\n",
    "\"\"\"\n",
    "from utils.config_parser import ConfigParser\n",
    "import data.threeDPW as module_data \n",
    "\n",
    "%aimport -ConfigParser # Due to an issue of pickle and auto_reload\n",
    "\n",
    "config = ConfigParser.wo_args(config=ospj(PROJECT_ROOT,'cfgs/project/local-config-hppw3d.json'))\n",
    "\n",
    "datamodule = config.init_obj('train_loader', module_data)\n",
    "train_loader = datamodule.get_loader()\n",
    "\n",
    "datamodule = config.init_obj('validation_loader', module_data)\n",
    "val_loader = datamodule.get_loader()\n",
    "\n",
    "datamodule = config.init_obj('test_loader', module_data)\n",
    "test_loader = datamodule.get_loader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72 79]\n",
      "[72 79]\n",
      "[72 79]\n",
      "[72 79]\n",
      "[72 79]\n",
      "[71 79]\n",
      "[71 79]\n",
      "[71 79]\n",
      "[71 79]\n",
      "[71 79]\n",
      "[71 79]\n",
      "[71 79]\n",
      "[72 79]\n",
      "[72 79]\n",
      "[72 79]\n",
      "[73 80]\n",
      "[74 80]\n",
      "[74 82]\n",
      "[74 83]\n",
      "[74 83]\n",
      "[74 84]\n",
      "[73 84]\n",
      "[72 85]\n",
      "[72 85]\n",
      "[71 85]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Visualize any loader\n",
    "\"\"\"\n",
    "%matplotlib qt\n",
    "from utils.viz import visualize_tfrecord_dataloader\n",
    "\n",
    "# Press q multiple times to exit. Maybe a better way possible.\n",
    "# Reduce batch size in the cfgs/project/*.json for the respective loader. \n",
    "visualize_tfrecord_dataloader(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 1, 3)\n",
      "(32, 10, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.zeros((32, 10, 3))\n",
    "\n",
    "\n",
    "print(np.expand_dims(array, axis=2).shape)\n",
    "print(np.expand_dims(array, axis=-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import *\n",
    "from models.vit.model import VisionTransformer\n",
    "print(os.getcwd())\n",
    "\n",
    "weights = torch.load(ospj(PROJECT_ROOT,'saved/models/vit.pth'))\n",
    "\n",
    "model = VisionTransformer(\n",
    "            image_size=224, # Input image size (width and height)\n",
    "            patch_size=16,  # Image broken into (16 x 16) non-overlaping batches\n",
    "            num_layers=4,  # Number of blocks in the Encoder\n",
    "            num_heads=12,   # Number of heads in each Multi-\"head\" attention\n",
    "            hidden_dim=768, # Token size (length of a single token)\n",
    "            mlp_dim=3072,   # Hidden layer size of each MLP layer,\n",
    "            total_layers=12,\n",
    "            global_pool=\"avg\"\n",
    "        )\n",
    "\n",
    "model.load_state_dict(weights, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: There's no GPU available on this machine,training will be performed on CPU.\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Initialize the trainer and model\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trainers.hppw3d_trainer import HPPW3DTrainer\n",
    "from utils.config_parser import ConfigParser\n",
    "import data.threeDPW as module_data \n",
    "\n",
    "%aimport -ConfigParser # Due to an issue of pickle and auto_reload\n",
    "\n",
    "config = ConfigParser.wo_args(config=ospj(PROJECT_ROOT,'cfgs/project/local-config-hppw3d.json'))\n",
    "trainer = HPPW3DTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n",
    "# stats = trainer.train()\n",
    "# plt.plot(stats['loss']['train'], label='train')\n",
    "# plt.plot(stats['loss']['val'], label='val')\n",
    "# plt.title('Classification loss history')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Classification loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 19, 2])\n",
      "torch.Size([1, 10, 19, 3])\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "for i, (history, future) in enumerate(test_loader):\n",
    "    img_seq = history[0].float()\n",
    "    history_pose_seq = history[1].float()\n",
    "    history_root_seq = history[2].float()\n",
    "    history_mask = history[3].float()\n",
    "    out_2d, out_3d, attentions = trainer.model(img_seq, history_pose_seq, history_root_seq, history_mask)\n",
    "    print(out_2d.shape)\n",
    "    print(out_3d.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m attentions[\u001b[39m0\u001b[39;49m][\u001b[39m2\u001b[39;49m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image_attens = attentions[0]\n",
    "pose_attens = attentions[1]\n",
    "decoder_attens = attentions[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                                                      Param #\n",
       "====================================================================================================\n",
       "HumanPosePredictorModel                                                     --\n",
       "├─PoseEncoder: 1-1                                                          --\n",
       "│    └─Sequential: 2-1                                                      --\n",
       "│    │    └─PoseEncoderBlock: 3-1                                           527,104\n",
       "│    │    └─PoseEncoderBlock: 3-2                                           527,104\n",
       "├─VisionTransformer: 1-2                                                    768\n",
       "│    └─Conv2d: 2-2                                                          590,592\n",
       "│    └─Encoder: 2-3                                                         151,296\n",
       "│    │    └─MultiInputSequential: 3-3                                       28,351,488\n",
       "│    │    └─LayerNorm: 3-4                                                  1,536\n",
       "│    └─Sequential: 2-4                                                      --\n",
       "│    │    └─Linear: 3-5                                                     769,000\n",
       "│    └─MaxPool2d: 2-5                                                       --\n",
       "├─TemporalEncoder: 1-3                                                      --\n",
       "│    └─MultiInputSequential: 2-6                                            --\n",
       "│    │    └─TemporalEncoderBlock: 3-6                                       9,462,528\n",
       "├─TemporalEncoder: 1-4                                                      --\n",
       "│    └─MultiInputSequential: 2-7                                            --\n",
       "│    │    └─TemporalEncoderBlock: 3-7                                       1,582,336\n",
       "├─PoseDecoder: 1-5                                                          --\n",
       "│    └─MultiheadAttention: 2-8                                              197,376\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-8                            65,792\n",
       "│    └─TransformerDecoderLayer: 2-9                                         --\n",
       "│    │    └─MultiheadAttention: 3-9                                         263,168\n",
       "│    │    └─MultiheadAttention: 3-10                                        263,168\n",
       "│    │    └─Linear: 3-11                                                    65,792\n",
       "│    │    └─Dropout: 3-12                                                   --\n",
       "│    │    └─Linear: 3-13                                                    65,792\n",
       "│    │    └─LayerNorm: 3-14                                                 512\n",
       "│    │    └─LayerNorm: 3-15                                                 512\n",
       "│    │    └─LayerNorm: 3-16                                                 512\n",
       "│    │    └─Dropout: 3-17                                                   --\n",
       "│    │    └─Dropout: 3-18                                                   --\n",
       "│    │    └─Dropout: 3-19                                                   --\n",
       "│    └─LayerNorm: 2-10                                                      512\n",
       "│    └─MLPBlock: 2-11                                                       --\n",
       "│    │    └─Linear: 3-20                                                    131,584\n",
       "│    │    └─GELU: 3-21                                                      --\n",
       "│    │    └─Dropout: 3-22                                                   --\n",
       "│    │    └─Linear: 3-23                                                    131,328\n",
       "│    │    └─Dropout: 3-24                                                   --\n",
       "│    └─TransformerDecoder: 2-12                                             --\n",
       "│    │    └─ModuleList: 3-25                                                1,318,912\n",
       "│    └─TemporalEncoder: 2-13                                                --\n",
       "│    │    └─MultiInputSequential: 3-26                                      1,055,232\n",
       "│    └─Linear: 2-14                                                         196,864\n",
       "├─FourierMLPEncoding: 1-6                                                   --\n",
       "│    └─Sequential: 2-15                                                     --\n",
       "│    │    └─Linear: 3-27                                                    49,664\n",
       "│    │    └─ReLU: 3-28                                                      --\n",
       "│    │    └─Linear: 3-29                                                    33,024\n",
       "├─FourierMLPEncoding: 1-7                                                   --\n",
       "│    └─Sequential: 2-16                                                     --\n",
       "│    │    └─Linear: 3-30                                                    98,816\n",
       "│    │    └─ReLU: 3-31                                                      --\n",
       "│    │    └─Linear: 3-32                                                    33,024\n",
       "├─Linear: 1-8                                                               771\n",
       "====================================================================================================\n",
       "Total params: 45,936,107\n",
       "Trainable params: 45,936,107\n",
       "Non-trainable params: 0\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "torchinfo.summary(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Perform test\n",
    "\"\"\"\n",
    "\n",
    "checkpoint_dir = '0604_104809' \n",
    "path = ospj(PROJECT_ROOT, f'saved/models/hppw/{checkpoint_dir}/best_val_model.pth')\n",
    "\n",
    "trainer.load_model(path=path)\n",
    "\n",
    "result = trainer.evaluate(loader=test_loader)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.temporal.encoder import TemporalEncoder, LocalTemporalEncoderBlock\n",
    "\n",
    "model = TemporalEncoder(\n",
    "    num_layers=3,\n",
    "    num_heads=8,\n",
    "    hidden_dim=256,\n",
    "    mlp_dim=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_feat = torch.randn(size=(32, 15, 197, 256))\n",
    "global_feat = torch.randn(size=(32, 15, 256))\n",
    "\n",
    "local_out, global_out = model(local_feat, global_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6335],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6336],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6336],\n",
       "         ...,\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6335],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6335],\n",
       "         [ 0.7680,  0.1498,  2.5715,  ..., -1.7233, -7.7865,  8.6336]],\n",
       "\n",
       "        [[-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         ...,\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401],\n",
       "         [-1.2756, -7.0257, -1.1117,  ...,  4.7989, -0.6455,  3.4401]],\n",
       "\n",
       "        [[-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         ...,\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397],\n",
       "         [-1.1115, -3.7773, -9.5381,  ..., -0.7923,  2.3916, -3.7397]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         ...,\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412],\n",
       "         [-8.3675,  3.4321,  6.9062,  ...,  4.1495,  4.4776, -3.2412]],\n",
       "\n",
       "        [[-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         ...,\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081],\n",
       "         [-3.0640, -2.3856, -8.3690,  ...,  7.7114,  0.1572, -4.7081]],\n",
       "\n",
       "        [[ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         ...,\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966],\n",
       "         [ 7.2000,  4.8411, 12.4626,  ..., -1.2724,  1.8068,  3.4966]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "├─MultiInputSequential: 1-1                        [-1, 197, 256]            --\n",
      "|    └─TemporalEncoderBlock: 2-1                   [-1, 14, 197, 256]        --\n",
      "|    |    └─LocalTemporalEncoderBlock: 3-1         [-1, 14, 197, 256]        528,128\n",
      "|    |    └─GlobalTemporalEncoderBlock: 3-2        [-1, 15, 256]             527,104\n",
      "|    └─TemporalEncoderBlock: 2-2                   [-1, 13, 197, 256]        --\n",
      "|    |    └─LocalTemporalEncoderBlock: 3-3         [-1, 13, 197, 256]        528,128\n",
      "|    |    └─GlobalTemporalEncoderBlock: 3-4        [-1, 15, 256]             527,104\n",
      "|    └─TemporalEncoderBlock: 2-3                   [-1, 197, 256]            --\n",
      "|    |    └─LocalTemporalEncoderBlock: 3-5         [-1, 197, 256]            528,128\n",
      "|    |    └─GlobalTemporalEncoderBlock: 3-6        [-1, 15, 256]             527,104\n",
      "====================================================================================================\n",
      "Total params: 3,165,696\n",
      "Trainable params: 3,165,696\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 21.54\n",
      "====================================================================================================\n",
      "Input size (MB): 92.81\n",
      "Forward/backward pass size (MB): 4.02\n",
      "Params size (MB): 12.08\n",
      "Estimated Total Size (MB): 108.91\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "├─MultiInputSequential: 1-1                        [-1, 197, 256]            --\n",
       "|    └─TemporalEncoderBlock: 2-1                   [-1, 14, 197, 256]        --\n",
       "|    |    └─LocalTemporalEncoderBlock: 3-1         [-1, 14, 197, 256]        528,128\n",
       "|    |    └─GlobalTemporalEncoderBlock: 3-2        [-1, 15, 256]             527,104\n",
       "|    └─TemporalEncoderBlock: 2-2                   [-1, 13, 197, 256]        --\n",
       "|    |    └─LocalTemporalEncoderBlock: 3-3         [-1, 13, 197, 256]        528,128\n",
       "|    |    └─GlobalTemporalEncoderBlock: 3-4        [-1, 15, 256]             527,104\n",
       "|    └─TemporalEncoderBlock: 2-3                   [-1, 197, 256]            --\n",
       "|    |    └─LocalTemporalEncoderBlock: 3-5         [-1, 197, 256]            528,128\n",
       "|    |    └─GlobalTemporalEncoderBlock: 3-6        [-1, 15, 256]             527,104\n",
       "====================================================================================================\n",
       "Total params: 3,165,696\n",
       "Trainable params: 3,165,696\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 21.54\n",
       "====================================================================================================\n",
       "Input size (MB): 92.81\n",
       "Forward/backward pass size (MB): 4.02\n",
       "Params size (MB): 12.08\n",
       "Estimated Total Size (MB): 108.91\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_data=[local_feat, global_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: There's no GPU available on this machine,training will be performed on CPU.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrainers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhppw_trainer\u001b[39;00m \u001b[39mimport\u001b[39;00m HPPWTrainer\n\u001b[1;32m----> 7\u001b[0m trainer \u001b[39m=\u001b[39m HPPWTrainer(config\u001b[39m=\u001b[39;49mconfig, train_loader\u001b[39m=\u001b[39;49mtrain_loader, eval_loader\u001b[39m=\u001b[39;49mval_loader)\n\u001b[0;32m      8\u001b[0m \u001b[39m# stats = trainer.train()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m plt\u001b[39m.\u001b[39mplot(stats[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\\../..\\src\\trainers\\hppw_trainer.py:24\u001b[0m, in \u001b[0;36mHPPWTrainer.__init__\u001b[1;34m(self, config, train_loader, eval_loader)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, train_loader, eval_loader\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    Create the model, loss criterion, optimizer, and dataloaders\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m    And anything else that might be needed during training. (e.g. device type)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config)    \n\u001b[0;32m     25\u001b[0m     \u001b[39m# build model architecture, then print to console\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39minit_obj(\u001b[39m'\u001b[39m\u001b[39march\u001b[39m\u001b[39m'\u001b[39m, module_arch)\n",
      "File \u001b[1;32md:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\\../..\\src\\trainers\\base.py:92\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39m# prepare for (multi-device) GPU training\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39m# This part doesn't do anything if you don't have a GPU.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_ids \u001b[39m=\u001b[39m prepare_device(config[\u001b[39m'\u001b[39m\u001b[39mn_gpu\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwandb_enabled \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39;49m\u001b[39mwandb\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwandb_enabled \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mwandb\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Saarland\\Research\\human-pose-prediction-in-the-wild\\src\\notebooks\\../..\\src\\utils\\config_parser.py:139\u001b[0m, in \u001b[0;36mConfigParser.__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m    138\u001b[0m     \u001b[39m\"\"\"Access items like ordinary dict.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[name]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'wandb'"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Initialize the trainer and model\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trainers.hppw_trainer import HPPWTrainer\n",
    "\n",
    "trainer = HPPWTrainer(config=config, train_loader=train_loader, eval_loader=val_loader)\n",
    "# stats = trainer.train()\n",
    "\n",
    "plt.plot(stats['loss']['train'], label='train')\n",
    "plt.plot(stats['loss']['val'], label='val')\n",
    "plt.title('Classification loss history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
