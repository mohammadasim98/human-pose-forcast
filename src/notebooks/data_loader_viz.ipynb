{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this line if you're using Colab to something like '/content/drive/MyDrive/TeamX/'\n",
    "# where TeamX is just the clone of repository on your Google Drive\n",
    "# and you have mounted the drive at /content/drive  \n",
    "# See the Tutorial Slides for more detail.\n",
    "\n",
    "# Works on your local machine but not on Colab!\n",
    "PROJECT_ROOT = '../..' \n",
    "\n",
    "# Fix this path and use this one on Colab\n",
    "# PROJECT_ROOT = '/content/drive/MyDrive/TeamX'\n",
    "\n",
    "import sys\n",
    "from os.path import join as ospj\n",
    "sys.path.append(ospj(PROJECT_ROOT, 'src'))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(765, 4, 4)\n",
      "(2, 765, 18, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing the dataset by displaying the scene and the corresponding poses directly from image and annotation files.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from utils.transforms import sample\n",
    "from utils.viz import annotate\n",
    "\n",
    "seq_name = \"courtyard_arguing_00\"\n",
    "seq_dir = ospj(PROJECT_ROOT,\"data\", \"3DPWPreprocessed\", \"sequenceFiles\")\n",
    "img_dir = ospj(PROJECT_ROOT,\"data\", \"3DPWPreprocessed\", \"imageFiles\")\n",
    "folder = \"train\"\n",
    "seq_path = ospj(seq_dir, folder, seq_name + \".npz\")\n",
    "\n",
    "seq = np.load(seq_path, allow_pickle=True)\n",
    "\n",
    "poses = seq[\"abs_poses\"]\n",
    "\n",
    "trans = seq[\"trans\"]\n",
    "jointPositions = seq[\"jointPositions\"]\n",
    "cam_ext = seq[\"extrinsics\"]\n",
    "cam_int = seq[\"intrinsics\"]\n",
    "pmask = seq[\"pmask\"]\n",
    "img_dir = os.path.join(img_dir, seq_name)    \n",
    "img_names = os.listdir(img_dir)\n",
    "\n",
    "cv2.namedWindow(f\"Padded Resized Image\", cv2.WINDOW_KEEPRATIO)  \n",
    "cv2.namedWindow(f\"Padding Mask\", cv2.WINDOW_KEEPRATIO)  \n",
    "print(cam_ext.shape)\n",
    "print(poses.shape)\n",
    "for id in range(poses.shape[1]):\n",
    "    img, _, root_joint, norm_pose, abs_pose = sample(id, poses, img_names, img_dir,  imsize=(384, 384, 3))    \n",
    "    img = annotate(img, poses[:, id, :, :], root_joint)\n",
    "    cv2.imshow(\"Padded Resized Image\", img)\n",
    "    cv2.imshow(\"Padding Mask\", pmask)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break    \n",
    "\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 4648\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "annotate_root() got an unexpected keyword argument 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m mask \u001b[39m=\u001b[39m history[\u001b[39m3\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     24\u001b[0m annoted_img \u001b[39m=\u001b[39m annotate_pose(img\u001b[39m=\u001b[39mimg, pose\u001b[39m=\u001b[39mabs_pose, color\u001b[39m=\u001b[39m(\u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), radius\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, thickness\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m annoted_img \u001b[39m=\u001b[39m annotate_root(img\u001b[39m=\u001b[39;49mannoted_img,root\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mexpand_dims(root_joint, \u001b[39m0\u001b[39;49m), color\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m255\u001b[39;49m, \u001b[39m0\u001b[39;49m), text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     26\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mHistory Image\u001b[39m\u001b[39m\"\u001b[39m, img)\n\u001b[0;32m     27\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mHistory Mask\u001b[39m\u001b[39m\"\u001b[39m, mask\u001b[39m*\u001b[39m\u001b[39m255\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: annotate_root() got an unexpected keyword argument 'text'"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing the dataset by displaying the scene and the corresponding poses from TFRecords.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from data.threeDPW import ThreeDPWTFRecordDataset\n",
    "from utils.transforms import cvt_absolute_pose\n",
    "from utils.viz import annotate_pose, annotate_root\n",
    "\n",
    "tfrecord_path = ospj(PROJECT_ROOT,\"data\", \"3DPWPreprocessedTFRecord\", \"train.tfrecord\")\n",
    "\n",
    "dataset = ThreeDPWTFRecordDataset(tfrecord_path, history_window=15, future_window=15, batch_size=1)\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "\n",
    "loader = dataset.get_loader()\n",
    "\n",
    "for i, (history, future) in enumerate(loader):\n",
    "    img = history[0][0][0].numpy()\n",
    "    norm_pose = history[1][0][0].numpy()\n",
    "    root_joint = history[2][0][0].numpy()\n",
    "    mask = history[3][0].numpy()\n",
    "    annoted_img = annotate_pose(img=img, pose=abs_pose, color=(255, 0, 0), radius=2, thickness=3, text=False)\n",
    "    annoted_img = annotate_root(img=annoted_img,root=np.expand_dims(root_joint, 0), color=(0, 255, 0))\n",
    "    cv2.imshow(\"History Image\", img)\n",
    "    cv2.imshow(\"History Mask\", mask*255)\n",
    "    \n",
    "    norm_pose = future[0][0][0].numpy()\n",
    "    root_joint = future[1][0][0].numpy()\n",
    "    abs_pose = cvt_absolute_pose(root_joint=np.expand_dims(root_joint, 0), norm_pose=np.expand_dims(norm_pose, 0))\n",
    "    annoted_img = annotate_pose(img=img, pose=abs_pose, color=(0, 255, 0), radius=2, thickness=3, text=False)\n",
    "    annoted_img = annotate_root(img=annoted_img,root=np.expand_dims(root_joint, 0), color=(0, 255, 0))\n",
    "    cv2.imshow(\"Future Image\", img)\n",
    "    cv2.imshow(\"Future Mask\", mask*255)\n",
    "    \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break     \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
